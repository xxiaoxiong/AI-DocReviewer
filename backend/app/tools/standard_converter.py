"""
æ ‡å‡†æ–‡æ¡£è½¬æ¢å·¥å…· - å°† Word/PDF æ ‡å‡†æ–‡æ¡£è½¬æ¢ä¸º JSON æ ¼å¼

ä½¿ç”¨åœºæ™¯ï¼š
1. ä½ æ‹¿åˆ°ä¸€ä»½ Word æ ¼å¼çš„æ ‡å‡†æ–‡æ¡£ï¼ˆå¦‚ï¼šã€Šè½¯ä»¶æµ‹è¯•è§„èŒƒ.docxã€‹ï¼‰
2. è¿è¡Œæ­¤å·¥å…·ï¼Œè‡ªåŠ¨æå–è§„åˆ™å¹¶ç”Ÿæˆ JSON
3. å¯é€‰ï¼šä½¿ç”¨ LLM è¾…åŠ©æå–ï¼ˆæ›´æ™ºèƒ½ï¼‰

è½¬æ¢ç­–ç•¥ï¼š
- åŸºç¡€ç‰ˆï¼šè§„åˆ™æ¨¡æ¿åŒ¹é…ï¼ˆå¿«é€Ÿï¼Œé€‚åˆæ ¼å¼è§„èŒƒçš„æ–‡æ¡£ï¼‰
- å¢å¼ºç‰ˆï¼šLLM è¾…åŠ©æå–ï¼ˆæ™ºèƒ½ï¼Œé€‚åˆå¤æ‚æ–‡æ¡£ï¼‰
"""
import json
import re
from pathlib import Path
from typing import List, Dict, Any, Optional
from loguru import logger
from docx import Document

from ..models.document import Standard, Category, Rule, CheckType, Severity


class StandardConverter:
    """æ ‡å‡†æ–‡æ¡£è½¬æ¢å™¨"""
    
    def __init__(self, use_llm: bool = False):
        """
        Args:
            use_llm: æ˜¯å¦ä½¿ç”¨ LLM è¾…åŠ©æå–ï¼ˆéœ€è¦é…ç½® LLMï¼‰
        """
        self.use_llm = use_llm
        self.llm_client = None
        
        if use_llm:
            # TODO: åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
            logger.info("ğŸ¤– LLM è¾…åŠ©æ¨¡å¼å·²å¯ç”¨")
    
    def convert_word_to_json(
        self,
        word_path: str,
        output_path: Optional[str] = None,
        protocol_id: Optional[str] = None,
        protocol_name: Optional[str] = None
    ) -> str:
        """
        å°† Word æ ‡å‡†æ–‡æ¡£è½¬æ¢ä¸º JSON
        
        Args:
            word_path: Word æ–‡æ¡£è·¯å¾„
            output_path: è¾“å‡º JSON è·¯å¾„ï¼ˆé»˜è®¤ï¼šåŒç›®å½•ä¸‹åŒå.jsonï¼‰
            protocol_id: åè®®IDï¼ˆé»˜è®¤ï¼šä»æ–‡ä»¶åæå–ï¼‰
            protocol_name: åè®®åç§°ï¼ˆé»˜è®¤ï¼šä»æ–‡æ¡£æ ‡é¢˜æå–ï¼‰
        
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        logger.info(f"ğŸ“„ å¼€å§‹è½¬æ¢æ ‡å‡†æ–‡æ¡£: {word_path}")
        
        # è§£æ Word æ–‡æ¡£
        doc = Document(word_path)
        
        # æå–å…ƒæ•°æ®
        if not protocol_id:
            protocol_id = Path(word_path).stem.upper().replace(" ", "_")
        
        if not protocol_name:
            protocol_name = self._extract_title(doc)
        
        logger.info(f"   åè®®ID: {protocol_id}")
        logger.info(f"   åè®®åç§°: {protocol_name}")
        
        # æå–è§„åˆ™
        categories = self._extract_rules(doc)
        
        # æ„å»ºæ ‡å‡†å¯¹è±¡
        standard = Standard(
            protocol_id=protocol_id,
            name=protocol_name,
            version="1.0",
            description=f"ä» {Path(word_path).name} è‡ªåŠ¨æå–",
            categories=categories
        )
        
        # ä¿å­˜ä¸º JSON
        if not output_path:
            output_path = str(Path(word_path).with_suffix('.json'))
        
        output_dir = Path(output_path).parent
        output_dir.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(
                standard.model_dump(),
                f,
                ensure_ascii=False,
                indent=2
            )
        
        logger.info(f"âœ… è½¬æ¢å®Œæˆ: {output_path}")
        logger.info(f"   å…±æå– {len(categories)} ä¸ªåˆ†ç±», {sum(len(c.rules) for c in categories)} æ¡è§„åˆ™")
        
        return output_path
    
    def _extract_title(self, doc: Document) -> str:
        """æå–æ–‡æ¡£æ ‡é¢˜"""
        for para in doc.paragraphs[:5]:  # åªçœ‹å‰5æ®µ
            text = para.text.strip()
            if text and len(text) < 50:
                # åˆ¤æ–­æ˜¯å¦æ˜¯æ ‡é¢˜æ ·å¼
                if para.style.name in ['Title', 'Heading 1'] or \
                   (para.runs and para.runs[0].bold and para.runs[0].font.size and para.runs[0].font.size.pt > 14):
                    return text
        
        return "æœªå‘½åæ ‡å‡†"
    
    def _extract_rules(self, doc: Document) -> List[Category]:
        """
        æå–è§„åˆ™ï¼ˆæ ¸å¿ƒé€»è¾‘ï¼‰
        
        ç­–ç•¥ï¼š
        1. è¯†åˆ«ç« èŠ‚æ ‡é¢˜ï¼ˆä½œä¸º Categoryï¼‰
        2. æå–è§„åˆ™æ¡ç›®ï¼ˆç¼–å· + æè¿°ï¼‰
        3. åˆ†æè§„åˆ™ç±»å‹å’Œå…³é”®è¯
        """
        categories = []
        current_category = None
        rule_counter = 1
        
        for para in doc.paragraphs:
            text = para.text.strip()
            if not text:
                continue
            
            # åˆ¤æ–­æ˜¯å¦æ˜¯ç« èŠ‚æ ‡é¢˜
            if self._is_section_heading(para, text):
                # ä¿å­˜ä¸Šä¸€ä¸ªåˆ†ç±»
                if current_category and current_category["rules"]:
                    categories.append(Category(**current_category))
                
                # åˆ›å»ºæ–°åˆ†ç±»
                current_category = {
                    "category": self._clean_section_title(text),
                    "rules": []
                }
                logger.debug(f"   å‘ç°ç« èŠ‚: {current_category['category']}")
            
            # åˆ¤æ–­æ˜¯å¦æ˜¯è§„åˆ™æ¡ç›®
            elif current_category and self._is_rule_item(text):
                rule = self._parse_rule(text, rule_counter)
                if rule:
                    current_category["rules"].append(rule)
                    rule_counter += 1
                    logger.debug(f"      æå–è§„åˆ™: {rule.description[:30]}...")
        
        # ä¿å­˜æœ€åä¸€ä¸ªåˆ†ç±»
        if current_category and current_category["rules"]:
            categories.append(Category(**current_category))
        
        return categories
    
    def _is_section_heading(self, para, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯ç« èŠ‚æ ‡é¢˜"""
        # æ–¹æ³•1ï¼šæ ·å¼åˆ¤æ–­
        if para.style.name.startswith('Heading'):
            return True
        
        # æ–¹æ³•2ï¼šæ ¼å¼åˆ¤æ–­ï¼ˆåŠ ç²— + å­—å·å¤§ï¼‰
        if para.runs:
            first_run = para.runs[0]
            if first_run.bold and first_run.font.size and first_run.font.size.pt >= 12:
                return True
        
        # æ–¹æ³•3ï¼šæ–‡æœ¬æ¨¡å¼åˆ¤æ–­
        # åŒ¹é…ï¼šä¸€ã€äºŒã€ä¸‰ æˆ– 1. 2. 3. æˆ– ç¬¬ä¸€ç« ã€ç¬¬äºŒèŠ‚
        patterns = [
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€ï¼]',  # ä¸€ã€
            r'^\d+[\.\ï¼ã€]',  # 1.
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚æ¡æ¬¾]',  # ç¬¬ä¸€ç« 
            r'^[\d\.]+\s+[^\d]',  # 1.1 æ ‡é¢˜
        ]
        
        for pattern in patterns:
            if re.match(pattern, text):
                # æ’é™¤è¿‡é•¿çš„æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯æ­£æ–‡ï¼‰
                if len(text) < 50:
                    return True
        
        return False
    
    def _clean_section_title(self, text: str) -> str:
        """æ¸…ç†ç« èŠ‚æ ‡é¢˜ï¼ˆå»é™¤ç¼–å·ï¼‰"""
        # å»é™¤å¸¸è§çš„ç¼–å·æ ¼å¼
        text = re.sub(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€ï¼]\s*', '', text)
        text = re.sub(r'^\d+[\.\ï¼ã€]\s*', '', text)
        text = re.sub(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚æ¡æ¬¾]\s*', '', text)
        text = re.sub(r'^[\d\.]+\s+', '', text)
        
        return text.strip()
    
    def _is_rule_item(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯è§„åˆ™æ¡ç›®"""
        # è§„åˆ™æ¡ç›®ç‰¹å¾ï¼š
        # 1. æœ‰ç¼–å·ï¼ˆå¦‚ï¼š1ï¼‰ã€ï¼ˆ1ï¼‰ã€â‘ ï¼‰
        # 2. åŒ…å«"åº”"ã€"å¿…é¡»"ã€"ä¸å¾—"ã€"åº”å½“"ç­‰å…³é”®è¯
        # 3. é•¿åº¦é€‚ä¸­ï¼ˆ20-200å­—ï¼‰
        
        if len(text) < 20 or len(text) > 500:
            return False
        
        # ç¼–å·æ¨¡å¼
        number_patterns = [
            r'^\d+[ï¼‰\)ã€ï¼]',  # 1ï¼‰ã€1)ã€1ã€ã€1.
            r'^[ï¼ˆ\(]\d+[ï¼‰\)]',  # (1)ã€ï¼ˆ1ï¼‰
            r'^[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]',  # â‘ 
        ]
        
        has_number = any(re.match(p, text) for p in number_patterns)
        
        # è§„åˆ™å…³é”®è¯
        rule_keywords = ['åº”', 'å¿…é¡»', 'ä¸å¾—', 'åº”å½“', 'éœ€è¦', 'è¦æ±‚', 'ç¦æ­¢', 'ä¸åº”', 'å®œ', 'å¯']
        has_keyword = any(kw in text for kw in rule_keywords)
        
        return has_number or has_keyword
    
    def _parse_rule(self, text: str, rule_id: int) -> Optional[Rule]:
        """è§£æå•æ¡è§„åˆ™"""
        try:
            # æ¸…ç†ç¼–å·
            description = self._clean_rule_number(text)
            
            # åˆ†æè§„åˆ™ç±»å‹
            check_type = self._infer_check_type(description)
            
            # æå–å…³é”®è¯
            keywords = self._extract_keywords(description)
            
            # åˆ¤æ–­ä¸¥é‡ç¨‹åº¦
            severity = self._infer_severity(description)
            
            return Rule(
                rule_id=f"R{rule_id:03d}",
                description=description,
                check_type=check_type,
                keywords=keywords,
                positive_examples=[],  # éœ€è¦æ‰‹åŠ¨è¡¥å……æˆ– LLM ç”Ÿæˆ
                negative_examples=[],
                severity=severity
            )
        except Exception as e:
            logger.warning(f"è§£æè§„åˆ™å¤±è´¥: {text[:50]}... - {e}")
            return None
    
    def _clean_rule_number(self, text: str) -> str:
        """æ¸…ç†è§„åˆ™ç¼–å·"""
        text = re.sub(r'^\d+[ï¼‰\)ã€ï¼]\s*', '', text)
        text = re.sub(r'^[ï¼ˆ\(]\d+[ï¼‰\)]\s*', '', text)
        text = re.sub(r'^[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]\s*', '', text)
        return text.strip()
    
    def _infer_check_type(self, text: str) -> CheckType:
        """æ¨æ–­æ£€æŸ¥ç±»å‹"""
        # æ ¼å¼ç±»å…³é”®è¯
        format_keywords = ['æ ¼å¼', 'å­—ä½“', 'å­—å·', 'æ ‡ç‚¹', 'ç¼©è¿›', 'å¯¹é½', 'é¡µè¾¹è·', 'è¡Œè·']
        if any(kw in text for kw in format_keywords):
            return CheckType.FORMAT
        
        # ç»“æ„ç±»å…³é”®è¯
        structure_keywords = ['ç»“æ„', 'ç« èŠ‚', 'ç›®å½•', 'é¡ºåº', 'å±‚æ¬¡', 'ç»„æˆ']
        if any(kw in text for kw in structure_keywords):
            return CheckType.STRUCTURE
        
        # é»˜è®¤ä¸ºè¯­ä¹‰ç±»
        return CheckType.SEMANTIC
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯ï¼ˆç®€å•ç‰ˆï¼‰"""
        # æå–åè¯å’ŒåŠ¨è¯ï¼ˆç®€åŒ–ç‰ˆï¼Œå¯ç”¨ jieba åˆ†è¯ä¼˜åŒ–ï¼‰
        keywords = []
        
        # å¸¸è§å…³é”®è¯æ¨¡å¼
        common_keywords = [
            'æ ‡é¢˜', 'æ­£æ–‡', 'æ‘˜è¦', 'å…³é”®è¯', 'ç›®å½•', 'é¡µç ', 'é¡µçœ‰', 'é¡µè„š',
            'å­—ä½“', 'å­—å·', 'åŠ ç²—', 'æ–œä½“', 'ä¸‹åˆ’çº¿', 'æ ‡ç‚¹', 'ç¼©è¿›', 'å¯¹é½',
            'å›¾è¡¨', 'è¡¨æ ¼', 'å…¬å¼', 'å¼•ç”¨', 'å‚è€ƒæ–‡çŒ®', 'é™„å½•',
            'å‡†ç¡®', 'ç®€æ´', 'å®Œæ•´', 'è§„èŒƒ', 'æ¸…æ™°', 'ä¸€è‡´'
        ]
        
        for kw in common_keywords:
            if kw in text:
                keywords.append(kw)
        
        return keywords[:5]  # æœ€å¤š5ä¸ª
    
    def _infer_severity(self, text: str) -> Severity:
        """æ¨æ–­ä¸¥é‡ç¨‹åº¦"""
        # é«˜ä¼˜å…ˆçº§å…³é”®è¯
        high_keywords = ['å¿…é¡»', 'ç¦æ­¢', 'ä¸å¾—', 'ä¸¥ç¦']
        if any(kw in text for kw in high_keywords):
            return Severity.HIGH
        
        # ä½ä¼˜å…ˆçº§å…³é”®è¯
        low_keywords = ['å®œ', 'å¯', 'å»ºè®®', 'æ¨è']
        if any(kw in text for kw in low_keywords):
            return Severity.LOW
        
        return Severity.MEDIUM
    
    def convert_with_llm(self, word_path: str) -> str:
        """
        ä½¿ç”¨ LLM è¾…åŠ©è½¬æ¢ï¼ˆæ›´æ™ºèƒ½ï¼‰
        
        TODO: å®ç° LLM è¾…åŠ©æå–
        - è‡ªåŠ¨è¯†åˆ«è§„åˆ™
        - ç”Ÿæˆæ­£åä¾‹
        - ä¼˜åŒ–æè¿°
        """
        raise NotImplementedError("LLM è¾…åŠ©è½¬æ¢åŠŸèƒ½å¾…å®ç°")


def main():
    """å‘½ä»¤è¡Œå·¥å…·"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ ‡å‡†æ–‡æ¡£è½¬æ¢å·¥å…·")
    parser.add_argument("input", help="è¾“å…¥ Word æ–‡æ¡£è·¯å¾„")
    parser.add_argument("-o", "--output", help="è¾“å‡º JSON è·¯å¾„")
    parser.add_argument("--id", help="åè®®ID")
    parser.add_argument("--name", help="åè®®åç§°")
    parser.add_argument("--llm", action="store_true", help="ä½¿ç”¨ LLM è¾…åŠ©")
    
    args = parser.parse_args()
    
    converter = StandardConverter(use_llm=args.llm)
    converter.convert_word_to_json(
        word_path=args.input,
        output_path=args.output,
        protocol_id=args.id,
        protocol_name=args.name
    )


if __name__ == "__main__":
    main()

