"""
API è·¯ç”± - æ–‡æ¡£å®¡æ ¸æ¥å£
"""
from fastapi import APIRouter, UploadFile, File, Form, HTTPException
from fastapi.responses import StreamingResponse
from typing import Optional
import json
import os
from pathlib import Path
from loguru import logger

from ..core.reviewer import DocumentReviewer
from ..core.rag_engine import RAGEngine
from ..core.rag_engine_v2 import RAGEngineV2
from ..services.llm_service import LLMService
from ..models.document import ReviewResult

router = APIRouter(prefix="/api/review", tags=["å®¡æ ¸"])

# åˆå§‹åŒ–æœåŠ¡ï¼ˆå…¨å±€å•ä¾‹ï¼‰
# è·å–é¡¹ç›®æ ¹ç›®å½• - ä»backendç›®å½•å‘ä¸Šä¸€çº§
backend_dir = Path(__file__).parent.parent.parent
project_root = backend_dir.parent
standards_dir = project_root / "standards" / "protocols"

# ğŸš€ ä½¿ç”¨æ–°ç‰ˆè¯­ä¹‰æ£€ç´¢å¼•æ“ï¼ˆå¯åˆ‡æ¢å›æ—§ç‰ˆï¼‰
USE_SEMANTIC_SEARCH = True  # è®¾ç½®ä¸º False ä½¿ç”¨æ—§ç‰ˆ TF-IDF

if USE_SEMANTIC_SEARCH:
    try:
        logger.info("ğŸš€ ä½¿ç”¨è¯­ä¹‰æ£€ç´¢å¼•æ“ V2 (BGE)")
        rag_engine = RAGEngineV2(standards_dir=str(standards_dir))
        
        # å°è¯•åŠ è½½å·²ä¿å­˜çš„ç´¢å¼•
        index_path = project_root / "standards" / "embeddings" / "index_v2.pkl"
        if index_path.exists():
            logger.info("ğŸ“¦ åŠ è½½å·²ä¿å­˜çš„ç´¢å¼•...")
            rag_engine.load_index(str(index_path))
        else:
            logger.info("ğŸ’¾ é¦–æ¬¡å¯åŠ¨ï¼Œä¿å­˜ç´¢å¼•...")
            rag_engine.save_index(str(index_path))
            
    except Exception as e:
        logger.warning(f"âš ï¸ è¯­ä¹‰æ£€ç´¢å¼•æ“åŠ è½½å¤±è´¥: {e}")
else:
    logger.info("ä½¿ç”¨ä¼ ç»Ÿæ£€ç´¢å¼•æ“ (TF-IDF)")
    rag_engine = RAGEngine(standards_dir=str(standards_dir))

llm_service = LLMService(use_local=False)  # é»˜è®¤ä½¿ç”¨ DeepSeek
reviewer = DocumentReviewer(rag_engine, llm_service)



@router.post("/document/stream")
async def review_document_stream(
    file: UploadFile = File(...),
    protocol_id: str = Form(...)
):
    """
    æµå¼å®¡æ ¸æ¥å£ï¼ˆå®æ—¶è¿”å›ç»“æœï¼‰
    
    é€‚åˆå¤§æ–‡æ¡£ï¼Œå¯ä»¥å®æ—¶çœ‹åˆ°å®¡æ ¸è¿›åº¦
    """
    if not file.filename.endswith(('.docx', '.doc')):
        raise HTTPException(status_code=400, detail="åªæ”¯æŒ Word æ–‡æ¡£")
    
    upload_dir = Path("data/uploads")
    upload_dir.mkdir(parents=True, exist_ok=True)
    file_path = upload_dir / file.filename
    
    with open(file_path, "wb") as f:
        content = await file.read()
        f.write(content)
    
    logger.info(f"æµå¼å®¡æ ¸å¼€å§‹: {file_path}, åè®®: {protocol_id}")
    
    # éªŒè¯åè®®æ˜¯å¦å­˜åœ¨
    available_protocols = rag_engine.list_available_protocols()
    protocol_ids = [p['protocol_id'] for p in available_protocols]
    if protocol_id not in protocol_ids:
        raise HTTPException(
            status_code=400, 
            detail=f"åè®® {protocol_id} ä¸å­˜åœ¨ã€‚å¯ç”¨åè®®: {', '.join(protocol_ids)}"
        )
    
    async def generate():
        """ç”Ÿæˆå™¨å‡½æ•° - æµå¼æ¨é€å®¡æ ¸ç»“æœ"""
        all_issues = []
        
        try:
            # 1. è§£ææ–‡æ¡£
            yield f"data: {json.dumps({'type': 'status', 'message': 'æ­£åœ¨è§£ææ–‡æ¡£...'}, ensure_ascii=False)}\n\n"
            
            doc_structure = reviewer.parser.parse_docx(str(file_path))
            chunks = reviewer.chunker.chunk_by_paragraphs(doc_structure)
            
            logger.info(f"æ–‡æ¡£åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªå—")
            
            # 2. æ™ºèƒ½ä¼˜åŒ–è¿‡æ»¤
            yield f"data: {json.dumps({'type': 'status', 'message': 'æ­£åœ¨æ™ºèƒ½ä¼˜åŒ–å®¡æ ¸ä»»åŠ¡...'}, ensure_ascii=False)}\n\n"
            
            chunks_to_review, optimization_info = reviewer.optimizer.filter_chunks_for_review(
                chunks, protocol_id, rag_engine
            )
            
            # å‘é€ä¼˜åŒ–ä¿¡æ¯
            opt_message = f'ä¼˜åŒ–å®Œæˆï¼š{optimization_info["original_count"]} ä¸ªå— -> {optimization_info["final_review_count"]} ä¸ªéœ€å®¡æ ¸ï¼ˆä¼˜åŒ–ç‡ {optimization_info["optimization_rate"]:.1f}%ï¼‰'
            yield f"data: {json.dumps({'type': 'optimization', 'data': optimization_info, 'message': opt_message}, ensure_ascii=False)}\n\n"
            
            # 3. å‘é€åˆå§‹åŒ–ä¿¡æ¯
            init_message = f'æ–‡æ¡£è§£æå®Œæˆï¼Œå…± {len(chunks)} ä¸ªæ®µè½ï¼Œéœ€å®¡æ ¸ {len(chunks_to_review)} ä¸ª'
            yield f"data: {json.dumps({'type': 'init', 'total_chunks': len(chunks), 'chunks_to_review': len(chunks_to_review), 'message': init_message}, ensure_ascii=False)}\n\n"
            
            # 4. é€å—å®¡æ ¸å¹¶å®æ—¶æ¨é€ï¼ˆåªå®¡æ ¸éœ€è¦çš„å—ï¼‰
            for i, chunk in enumerate(chunks_to_review):
                try:
                    # å‘é€è¿›åº¦
                    progress_message = f'æ­£åœ¨å®¡æ ¸ç¬¬ {i+1}/{len(chunks_to_review)} æ®µ...'
                    yield f"data: {json.dumps({'type': 'progress', 'current': i + 1, 'total': len(chunks_to_review), 'message': progress_message}, ensure_ascii=False)}\n\n"
                    
                    # å®¡æ ¸å½“å‰å—
                    issues = await reviewer._review_chunk(chunk, protocol_id)
                    
                    # æ”¶é›†æ‰€æœ‰é—®é¢˜
                    all_issues.extend(issues)
                    
                    # å¦‚æœæœ‰é—®é¢˜ï¼Œç«‹å³æ¨é€
                    if issues:
                        for issue in issues:
                            yield f"data: {json.dumps({'type': 'issue', 'data': issue.dict()}, ensure_ascii=False)}\n\n"
                    
                except Exception as e:
                    logger.error(f"å®¡æ ¸å— {i} å¤±è´¥: {e}")
                    error_message = f'å®¡æ ¸ç¬¬ {i+1} æ®µæ—¶å‡ºé”™: {str(e)}'
                    yield f"data: {json.dumps({'type': 'error', 'message': error_message}, ensure_ascii=False)}\n\n"
            
            # 5. å¤„ç†ç¼“å­˜çš„ç»“æœ
            for chunk_id, cached_issues in optimization_info.get('cached_results', {}).items():
                all_issues.extend(cached_issues)
                if cached_issues:
                    for issue in cached_issues:
                        yield f"data: {json.dumps({'type': 'issue', 'data': issue.dict()}, ensure_ascii=False)}\n\n"
            
            # 6. å»é‡å’Œç”Ÿæˆæ‘˜è¦
            unique_issues = reviewer._deduplicate_issues(all_issues)
            unique_issues.sort(key=lambda x: (x.page or 0, x.position))
            summary = reviewer._generate_summary(unique_issues)
            
            # 7. è·å–ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯
            optimizer_stats = reviewer.optimizer.get_statistics()
            optimizer_stats['cache_size'] = reviewer.optimizer.get_cache_size()
            
            # 8. å‘é€å®Œæˆä¿¡å·
            yield f"data: {json.dumps({'type': 'complete', 'total_issues': len(unique_issues), 'summary': summary, 'optimization_info': optimization_info, 'optimizer_stats': optimizer_stats, 'message': 'å®¡æ ¸å®Œæˆï¼'}, ensure_ascii=False)}\n\n"
            
            logger.info(f"æµå¼å®¡æ ¸å®Œæˆ: å‘ç° {len(unique_issues)} ä¸ªé—®é¢˜ï¼Œä¼˜åŒ–ç‡ {optimization_info['optimization_rate']:.1f}%")
        
        except Exception as e:
            logger.error(f"æµå¼å®¡æ ¸å¤±è´¥: {e}")
            import traceback
            error_trace = traceback.format_exc()
            logger.error(f"è¯¦ç»†é”™è¯¯:\n{error_trace}")
            
            error_message = f'å®¡æ ¸å¤±è´¥: {str(e)}'
            yield f"data: {json.dumps({'type': 'error', 'message': error_message}, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


@router.get("/protocols")
async def list_protocols():
    """
    åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„åè®®
    
    Returns:
        åè®®åˆ—è¡¨
    """
    try:
        protocols = rag_engine.list_available_protocols()
        return {
            "total": len(protocols),
            "protocols": protocols
        }
    except Exception as e:
        logger.error(f"è·å–åè®®åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/protocols/{protocol_id}/rules")
async def get_protocol_rules(protocol_id: str):
    """
    è·å–æŒ‡å®šåè®®çš„æ‰€æœ‰è§„åˆ™
    
    Args:
        protocol_id: åè®®ID
    
    Returns:
        è§„åˆ™åˆ—è¡¨
    """
    try:
        rules = rag_engine.get_all_rules_by_protocol(protocol_id)
        return {
            "protocol_id": protocol_id,
            "total_rules": len(rules),
            "rules": rules
        }
    except Exception as e:
        logger.error(f"è·å–åè®®è§„åˆ™å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/logs/recent")
async def get_recent_logs(limit: int = 10):
    """
    è·å–æœ€è¿‘çš„å®¡æ ¸æ—¥å¿—
    
    Args:
        limit: è¿”å›æ•°é‡
    
    Returns:
        æ—¥å¿—åˆ—è¡¨
    """
    try:
        from ..core.review_logger import review_logger
        sessions = review_logger.get_recent_sessions(limit)
        return {
            "total": len(sessions),
            "sessions": sessions
        }
    except Exception as e:
        logger.error(f"è·å–å®¡æ ¸æ—¥å¿—å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/logs/{session_id}")
async def get_session_log(session_id: str):
    """
    è·å–æŒ‡å®šä¼šè¯çš„è¯¦ç»†æ—¥å¿—
    
    Args:
        session_id: ä¼šè¯ID
    
    Returns:
        è¯¦ç»†æ—¥å¿—
    """
    try:
        from ..core.review_logger import review_logger
        log_file = review_logger.log_dir / f"{session_id}_full.json"
        
        if not log_file.exists():
            raise HTTPException(status_code=404, detail="æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨")
        
        import json
        with open(log_file, 'r', encoding='utf-8') as f:
            log_data = json.load(f)
        
        return log_data
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è¯»å–æ—¥å¿—å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/document/preview")
async def preview_document_chunks(
    file: UploadFile = File(..., description="å¾…é¢„è§ˆçš„ Word æ–‡æ¡£")
):
    """
    é¢„è§ˆæ–‡æ¡£åˆ†å—æƒ…å†µï¼ˆä¸è¿›è¡Œå®¡æ ¸ï¼‰
    
    ç”¨äºè°ƒè¯•å’Œå¯è§†åŒ–åˆ†å—æ•ˆæœ
    
    Returns:
        åˆ†å—ä¿¡æ¯
    """
    if not file.filename.endswith(('.docx', '.doc')):
        raise HTTPException(status_code=400, detail="åªæ”¯æŒ Word æ–‡æ¡£")
    
    upload_dir = Path("data/uploads")
    upload_dir.mkdir(parents=True, exist_ok=True)
    file_path = upload_dir / file.filename
    
    try:
        # ä¿å­˜æ–‡ä»¶
        with open(file_path, "wb") as f:
            content = await file.read()
            f.write(content)
        
        logger.info(f"é¢„è§ˆæ–‡æ¡£åˆ†å—: {file_path}")
        
        # è§£ææ–‡æ¡£
        doc_structure = reviewer.parser.parse_docx(str(file_path))
        
        # åˆ†å—
        chunks = reviewer.chunker.chunk_by_paragraphs(doc_structure)
        
        # æ„é€ è¿”å›æ•°æ®
        chunks_info = []
        for chunk in chunks:
            chunks_info.append({
                "chunk_id": chunk.chunk_id,
                "text": chunk.text,
                "text_length": len(chunk.text),
                "section": chunk.section,
                "context_before": chunk.context_before,
                "context_after": chunk.context_after,
                "start_pos": chunk.start_pos,
                "end_pos": chunk.end_pos
            })
        
        return {
            "filename": file.filename,
            "total_chunks": len(chunks),
            "total_paragraphs": doc_structure["metadata"]["total_paragraphs"],
            "total_chars": doc_structure["metadata"]["total_chars"],
            "chunks": chunks_info,
            "document_structure": {
                "title": doc_structure["title"],
                "sections": [
                    {
                        "level": s["level"],
                        "title": s["title"],
                        "paragraph_count": len(s["paragraphs"])
                    }
                    for s in doc_structure["sections"]
                ]
            }
        }
    
    except Exception as e:
        logger.error(f"é¢„è§ˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/document/test-retrieval")
async def test_retrieval(
    text: str = Form(..., description="æµ‹è¯•æ–‡æœ¬"),
    protocol_id: str = Form(..., description="åè®®ID"),
    top_k: int = Form(5, description="è¿”å›æ•°é‡")
):
    """
    æµ‹è¯•RAGæ£€ç´¢æ•ˆæœ
    
    ç”¨äºè°ƒè¯•å’Œç†è§£æ£€ç´¢æœºåˆ¶
    
    Returns:
        æ£€ç´¢åˆ°çš„è§„åˆ™åŠç›¸ä¼¼åº¦
    """
    try:
        logger.info(f"æµ‹è¯•æ£€ç´¢: æ–‡æœ¬='{text[:50]}...', åè®®={protocol_id}")
        
        # æ£€ç´¢è§„åˆ™
        relevant_rules = rag_engine.retrieve_relevant_rules(
            text=text,
            protocol_id=protocol_id,
            top_k=top_k,
            use_hybrid=True,
            min_similarity=0.0  # ä¸è¿‡æ»¤ï¼Œæ˜¾ç¤ºæ‰€æœ‰ç»“æœ
        )
        
        return {
            "query_text": text,
            "protocol_id": protocol_id,
            "total_matched": len(relevant_rules),
            "rules": relevant_rules
        }
    
    except Exception as e:
        logger.error(f"æ£€ç´¢æµ‹è¯•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/vectors/info")
async def get_vector_info():
    """
    è·å–å‘é‡ç´¢å¼•ä¿¡æ¯ï¼ˆè¯æ˜å‘é‡åŒ–ç¡®å®å­˜åœ¨ï¼‰
    
    Returns:
        å‘é‡ç´¢å¼•çš„è¯¦ç»†ä¿¡æ¯
    """
    try:
        if rag_engine.rule_vectors is None:
            return {
                "status": "not_initialized",
                "message": "å‘é‡ç´¢å¼•æœªåˆå§‹åŒ–"
            }
        
        import numpy as np
        
        # è·å–å‘é‡ä¿¡æ¯
        vector_shape = rag_engine.rule_vectors.shape
        vector_dtype = str(rag_engine.rule_vectors.dtype)
        
        # è®¡ç®—ä¸€äº›ç»Ÿè®¡ä¿¡æ¯
        vector_mean = float(np.mean(rag_engine.rule_vectors))
        vector_std = float(np.std(rag_engine.rule_vectors))
        vector_min = float(np.min(rag_engine.rule_vectors))
        vector_max = float(np.max(rag_engine.rule_vectors))
        
        # è·å–ç¬¬ä¸€æ¡è§„åˆ™çš„å‘é‡ä½œä¸ºç¤ºä¾‹
        first_vector = rag_engine.rule_vectors[0].tolist()[:10]  # åªæ˜¾ç¤ºå‰10ç»´
        
        return {
            "status": "initialized",
            "model_name": rag_engine.model_name,
            "total_rules": len(rag_engine.rule_index),
            "vector_dimension": vector_shape[1],
            "vector_shape": list(vector_shape),
            "vector_dtype": vector_dtype,
            "statistics": {
                "mean": vector_mean,
                "std": vector_std,
                "min": vector_min,
                "max": vector_max
            },
            "sample_vector": {
                "rule_id": rag_engine.rule_index[0]["rule"].rule_id,
                "description": rag_engine.rule_index[0]["rule"].description,
                "vector_preview": first_vector,
                "note": "åªæ˜¾ç¤ºå‰10ç»´ï¼Œå®Œæ•´å‘é‡æœ‰768ç»´"
            },
            "faiss_enabled": rag_engine.use_faiss and rag_engine.faiss_index is not None
        }
    
    except Exception as e:
        logger.error(f"è·å–å‘é‡ä¿¡æ¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/optimizer/clear-cache")
async def clear_optimizer_cache():
    """
    æ¸…ç©ºä¼˜åŒ–å™¨ç¼“å­˜
    
    ç”¨äºé‡Šæ”¾å†…å­˜æˆ–é‡ç½®ç¼“å­˜
    
    Returns:
        æ“ä½œç»“æœ
    """
    try:
        old_size = reviewer.optimizer.get_cache_size()
        reviewer.optimizer.clear_cache()
        
        logger.info(f"ç¼“å­˜å·²æ¸…ç©º: æ¸…é™¤äº† {old_size} ä¸ªç¼“å­˜é¡¹")
        
        return {
            "success": True,
            "message": f"ç¼“å­˜å·²æ¸…ç©º",
            "cleared_items": old_size
        }
    except Exception as e:
        logger.error(f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/optimizer/stats")
async def get_optimizer_stats():
    """
    è·å–ä¼˜åŒ–å™¨ç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        ä¼˜åŒ–å™¨çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        stats = reviewer.optimizer.get_statistics()
        stats['cache_size'] = reviewer.optimizer.get_cache_size()
        
        return {
            "success": True,
            "statistics": stats
        }
    except Exception as e:
        logger.error(f"è·å–ä¼˜åŒ–å™¨ç»Ÿè®¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))