"""
API è·¯ç”± - æ–‡æ¡£å®¡æ ¸æ¥å£
"""
from fastapi import APIRouter, UploadFile, File, Form, HTTPException
from fastapi.responses import StreamingResponse
from typing import Optional
import json
import os
from pathlib import Path
from loguru import logger

from ..core.reviewer import DocumentReviewer
from ..core.rag_engine import RAGEngine
from ..core.rag_engine_v2 import RAGEngineV2
from ..services.llm_service import LLMService
from ..models.document import ReviewResult

router = APIRouter(prefix="/api/review", tags=["å®¡æ ¸"])

# åˆå§‹åŒ–æœåŠ¡ï¼ˆå…¨å±€å•ä¾‹ï¼‰
# è·å–é¡¹ç›®æ ¹ç›®å½• - ä»backendç›®å½•å‘ä¸Šä¸€çº§
backend_dir = Path(__file__).parent.parent.parent
project_root = backend_dir.parent
standards_dir = project_root / "standards" / "protocols"

# ğŸš€ ä½¿ç”¨æ–°ç‰ˆè¯­ä¹‰æ£€ç´¢å¼•æ“ï¼ˆå¯åˆ‡æ¢å›æ—§ç‰ˆï¼‰
USE_SEMANTIC_SEARCH = True  # è®¾ç½®ä¸º False ä½¿ç”¨æ—§ç‰ˆ TF-IDF

if USE_SEMANTIC_SEARCH:
    try:
        logger.info("ğŸš€ ä½¿ç”¨è¯­ä¹‰æ£€ç´¢å¼•æ“ V2 (BGE)")
        rag_engine = RAGEngineV2(standards_dir=str(standards_dir))
    except Exception as e:
        logger.warning(f"âš ï¸ è¯­ä¹‰æ£€ç´¢å¼•æ“åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ° TF-IDF: {e}")
        rag_engine = RAGEngine(standards_dir=str(standards_dir))
else:
    logger.info("ä½¿ç”¨ä¼ ç»Ÿæ£€ç´¢å¼•æ“ (TF-IDF)")
    rag_engine = RAGEngine(standards_dir=str(standards_dir))

llm_service = LLMService(use_local=False)  # é»˜è®¤ä½¿ç”¨ DeepSeek
reviewer = DocumentReviewer(rag_engine, llm_service)


@router.post("/document", response_model=ReviewResult)
async def review_document(
    file: UploadFile = File(..., description="å¾…å®¡æ ¸çš„ Word æ–‡æ¡£"),
    protocol_id: str = Form(..., description="ä½¿ç”¨çš„åè®®ID")
):
    """
    å®¡æ ¸æ–‡æ¡£æ¥å£
    
    Args:
        file: Word æ–‡æ¡£
        protocol_id: åè®®IDï¼ˆå¦‚ï¼šGB_T_9704_2012ï¼‰
    
    Returns:
        å®¡æ ¸ç»“æœ
    """
    # éªŒè¯æ–‡ä»¶ç±»å‹
    if not file.filename.endswith(('.docx', '.doc')):
        raise HTTPException(status_code=400, detail="åªæ”¯æŒ Word æ–‡æ¡£ï¼ˆ.docx, .docï¼‰")
    
    # ä¿å­˜ä¸Šä¼ æ–‡ä»¶
    upload_dir = Path("data/uploads")
    upload_dir.mkdir(parents=True, exist_ok=True)
    
    file_path = upload_dir / file.filename
    
    try:
        with open(file_path, "wb") as f:
            content = await file.read()
            f.write(content)
        
        logger.info(f"æ–‡ä»¶å·²ä¿å­˜: {file_path}")
        logger.info(f"ä½¿ç”¨åè®®: {protocol_id}")
        
        # éªŒè¯åè®®æ˜¯å¦å­˜åœ¨
        available_protocols = rag_engine.list_available_protocols()
        protocol_ids = [p['protocol_id'] for p in available_protocols]
        if protocol_id not in protocol_ids:
            raise HTTPException(
                status_code=400, 
                detail=f"åè®® {protocol_id} ä¸å­˜åœ¨ã€‚å¯ç”¨åè®®: {', '.join(protocol_ids)}"
            )
        
        # å®¡æ ¸æ–‡æ¡£
        logger.info("=" * 80)
        logger.info("ğŸš€ å¼€å§‹è°ƒç”¨å®¡æ ¸å™¨...")
        logger.info("=" * 80)
        
        result = await reviewer.review_document(
            file_path=str(file_path),
            protocol_id=protocol_id
        )
        
        logger.info("=" * 80)
        logger.info(f"âœ… å®¡æ ¸å®Œæˆï¼Œå‘ç° {result.total_issues} ä¸ªé—®é¢˜")
        logger.info("=" * 80)
        
        return result
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error("=" * 80)
        logger.error(f"âŒ å®¡æ ¸å¤±è´¥: {e}")
        logger.error("=" * 80)
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        
        import traceback
        error_trace = traceback.format_exc()
        
        # æä¾›æ›´å‹å¥½çš„é”™è¯¯æç¤º
        error_msg = str(e)
        if "API" in error_msg or "key" in error_msg.lower():
            error_msg = f"API è°ƒç”¨å¤±è´¥: {error_msg}\n\nè¯·æ£€æŸ¥ï¼š\n1. DeepSeek API Key æ˜¯å¦æ­£ç¡®\n2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n3. API é…é¢æ˜¯å¦å……è¶³"
        elif "timeout" in error_msg.lower():
            error_msg = f"è¯·æ±‚è¶…æ—¶: {error_msg}\n\nå¯èƒ½åŸå› ï¼š\n1. ç½‘ç»œè¿æ¥ä¸ç¨³å®š\n2. æ–‡æ¡£è¿‡å¤§\n3. API æœåŠ¡å“åº”æ…¢"
        
        raise HTTPException(status_code=500, detail=error_msg)
    
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        if file_path.exists():
            # os.remove(file_path)  # å–æ¶ˆæ³¨é‡Šä»¥è‡ªåŠ¨åˆ é™¤
            pass


@router.post("/document/stream")
async def review_document_stream(
    file: UploadFile = File(...),
    protocol_id: str = Form(...)
):
    """
    æµå¼å®¡æ ¸æ¥å£ï¼ˆå®æ—¶è¿”å›ç»“æœï¼‰
    
    é€‚åˆå¤§æ–‡æ¡£ï¼Œå¯ä»¥å®æ—¶çœ‹åˆ°å®¡æ ¸è¿›åº¦
    """
    if not file.filename.endswith(('.docx', '.doc')):
        raise HTTPException(status_code=400, detail="åªæ”¯æŒ Word æ–‡æ¡£")
    
    upload_dir = Path("data/uploads")
    upload_dir.mkdir(parents=True, exist_ok=True)
    file_path = upload_dir / file.filename
    
    try:
        with open(file_path, "wb") as f:
            content = await file.read()
            f.write(content)
        
        logger.info(f"æµå¼å®¡æ ¸å¼€å§‹: {file_path}, åè®®: {protocol_id}")
        
        # éªŒè¯åè®®æ˜¯å¦å­˜åœ¨
        available_protocols = rag_engine.list_available_protocols()
        protocol_ids = [p['protocol_id'] for p in available_protocols]
        if protocol_id not in protocol_ids:
            raise HTTPException(
                status_code=400, 
                detail=f"åè®® {protocol_id} ä¸å­˜åœ¨ã€‚å¯ç”¨åè®®: {', '.join(protocol_ids)}"
            )
        
        async def generate():
            """ç”Ÿæˆå™¨å‡½æ•° - æµå¼æ¨é€å®¡æ ¸ç»“æœ"""
            all_issues = []
            
            try:
                # 1. è§£ææ–‡æ¡£
                yield f"data: {json.dumps({'type': 'status', 'message': 'æ­£åœ¨è§£ææ–‡æ¡£...'}, ensure_ascii=False)}\n\n"
                
                doc_structure = reviewer.parser.parse_docx(str(file_path))
                chunks = reviewer.chunker.chunk_by_paragraphs(doc_structure)
                
                logger.info(f"æ–‡æ¡£åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªå—")
                
                # 2. å‘é€åˆå§‹åŒ–ä¿¡æ¯
                yield f"data: {json.dumps({'type': 'init', 'total_chunks': len(chunks), 'message': f'æ–‡æ¡£è§£æå®Œæˆï¼Œå…± {len(chunks)} ä¸ªæ®µè½'}, ensure_ascii=False)}\n\n"
                
                # 3. é€å—å®¡æ ¸å¹¶å®æ—¶æ¨é€
                for i, chunk in enumerate(chunks):
                    try:
                        # å‘é€è¿›åº¦
                        yield f"data: {json.dumps({'type': 'progress', 'current': i + 1, 'total': len(chunks), 'message': f'æ­£åœ¨å®¡æ ¸ç¬¬ {i+1}/{len(chunks)} æ®µ...'}, ensure_ascii=False)}\n\n"
                        
                        # å®¡æ ¸å½“å‰å—
                        issues = await reviewer._review_chunk(chunk, protocol_id)
                        
                        # æ”¶é›†æ‰€æœ‰é—®é¢˜
                        all_issues.extend(issues)
                        
                        # å¦‚æœæœ‰é—®é¢˜ï¼Œç«‹å³æ¨é€
                        if issues:
                            for issue in issues:
                                yield f"data: {json.dumps({'type': 'issue', 'data': issue.dict()}, ensure_ascii=False)}\n\n"
                        
                    except Exception as e:
                        logger.error(f"å®¡æ ¸å— {i} å¤±è´¥: {e}")
                        yield f"data: {json.dumps({'type': 'error', 'message': f'å®¡æ ¸ç¬¬ {i+1} æ®µæ—¶å‡ºé”™: {str(e)}'}, ensure_ascii=False)}\n\n"
                
                # 4. å»é‡å’Œç”Ÿæˆæ‘˜è¦
                unique_issues = reviewer._deduplicate_issues(all_issues)
                unique_issues.sort(key=lambda x: (x.page or 0, x.position))
                summary = reviewer._generate_summary(unique_issues)
                
                # 5. å‘é€å®Œæˆä¿¡å·
                yield f"data: {json.dumps({'type': 'complete', 'total_issues': len(unique_issues), 'summary': summary, 'message': 'å®¡æ ¸å®Œæˆï¼'}, ensure_ascii=False)}\n\n"
                
                logger.info(f"æµå¼å®¡æ ¸å®Œæˆ: å‘ç° {len(unique_issues)} ä¸ªé—®é¢˜")
            
            except Exception as e:
                logger.error(f"æµå¼å®¡æ ¸å¤±è´¥: {e}")
                import traceback
                error_trace = traceback.format_exc()
                logger.error(f"è¯¦ç»†é”™è¯¯:\n{error_trace}")
                
                yield f"data: {json.dumps({'type': 'error', 'message': f'å®¡æ ¸å¤±è´¥: {str(e)}'}, ensure_ascii=False)}\n\n"
        
        return StreamingResponse(
            generate(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )
    
    except Exception as e:
        logger.error(f"æµå¼å®¡æ ¸åˆå§‹åŒ–å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/protocols")
async def list_protocols():
    """
    åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„åè®®
    
    Returns:
        åè®®åˆ—è¡¨
    """
    try:
        protocols = rag_engine.list_available_protocols()
        return {
            "total": len(protocols),
            "protocols": protocols
        }
    except Exception as e:
        logger.error(f"è·å–åè®®åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/protocols/{protocol_id}/rules")
async def get_protocol_rules(protocol_id: str):
    """
    è·å–æŒ‡å®šåè®®çš„æ‰€æœ‰è§„åˆ™
    
    Args:
        protocol_id: åè®®ID
    
    Returns:
        è§„åˆ™åˆ—è¡¨
    """
    try:
        rules = rag_engine.get_all_rules_by_protocol(protocol_id)
        return {
            "protocol_id": protocol_id,
            "total_rules": len(rules),
            "rules": rules
        }
    except Exception as e:
        logger.error(f"è·å–åè®®è§„åˆ™å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/logs/recent")
async def get_recent_logs(limit: int = 10):
    """
    è·å–æœ€è¿‘çš„å®¡æ ¸æ—¥å¿—
    
    Args:
        limit: è¿”å›æ•°é‡
    
    Returns:
        æ—¥å¿—åˆ—è¡¨
    """
    try:
        from ..core.review_logger import review_logger
        sessions = review_logger.get_recent_sessions(limit)
        return {
            "total": len(sessions),
            "sessions": sessions
        }
    except Exception as e:
        logger.error(f"è·å–å®¡æ ¸æ—¥å¿—å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/logs/{session_id}")
async def get_session_log(session_id: str):
    """
    è·å–æŒ‡å®šä¼šè¯çš„è¯¦ç»†æ—¥å¿—
    
    Args:
        session_id: ä¼šè¯ID
    
    Returns:
        è¯¦ç»†æ—¥å¿—
    """
    try:
        from ..core.review_logger import review_logger
        log_file = review_logger.log_dir / f"{session_id}_full.json"
        
        if not log_file.exists():
            raise HTTPException(status_code=404, detail="æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨")
        
        import json
        with open(log_file, 'r', encoding='utf-8') as f:
            log_data = json.load(f)
        
        return log_data
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è¯»å–æ—¥å¿—å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))