"""
LLM æœåŠ¡ - æ”¯æŒ DeepSeek å’Œæœ¬åœ°æ¨¡å‹
"""
import httpx
import json
from typing import Dict, Any, Optional, List
from loguru import logger
from tenacity import retry, stop_after_attempt, wait_exponential

from ..config import settings


class LLMService:
    """å¤§è¯­è¨€æ¨¡å‹æœåŠ¡"""
    
    def __init__(self, use_local: bool = False):
        """
        åˆå§‹åŒ– LLM æœåŠ¡
        
        Args:
            use_local: æ˜¯å¦ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆå†…ç½‘éƒ¨ç½²çš„å°æ¨¡å‹ï¼‰
        """
        self.use_local = use_local
        
        if use_local and settings.local_model_api_base:
            self.api_base = settings.local_model_api_base
            self.model = settings.local_model_name
            self.api_key = "dummy"  # æœ¬åœ°æ¨¡å‹å¯èƒ½ä¸éœ€è¦ key
            logger.info(f"ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {self.model}")
        else:
            self.api_base = settings.deepseek_api_base
            self.model = settings.deepseek_model
            self.api_key = settings.deepseek_api_key
            
            # æ£€æŸ¥ API Key æ˜¯å¦é…ç½®
            if not self.api_key or self.api_key.strip() == "":
                logger.error("âŒ DeepSeek API Key æœªé…ç½®ï¼")
                logger.error("è¯·åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º .env æ–‡ä»¶ï¼Œæ·»åŠ ï¼š")
                logger.error("DEEPSEEK_API_KEY=your_api_key_here")
                raise ValueError("DeepSeek API Key æœªé…ç½®ï¼Œæ— æ³•ä½¿ç”¨ LLM æœåŠ¡")
            
            logger.info(f"âœ… ä½¿ç”¨ DeepSeek æ¨¡å‹: {self.model}")
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10)
    )
    async def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.1,  # ä½æ¸©åº¦ä¿è¯ç¨³å®šæ€§
        max_tokens: int = 2000,
        response_format: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨ LLM API
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
            response_format: å“åº”æ ¼å¼ï¼ˆå¦‚ {"type": "json_object"}ï¼‰
        
        Returns:
            API å“åº”
        """
        import time
        start_time = time.time()
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens
        }
        
        # DeepSeek æ”¯æŒ JSON æ¨¡å¼
        if response_format:
            payload["response_format"] = response_format
        
        logger.info(f"ğŸ¤– è°ƒç”¨ LLM API: {self.model}")
        logger.debug(f"   - Prompt é•¿åº¦: {len(messages[-1]['content'])} å­—ç¬¦")
        
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    f"{self.api_base}/chat/completions",
                    headers=headers,
                    json=payload
                )
                
                elapsed = time.time() - start_time
                
                if response.status_code == 200:
                    result = response.json()
                    
                    # æå–ä½¿ç”¨ä¿¡æ¯
                    usage = result.get("usage", {})
                    prompt_tokens = usage.get("prompt_tokens", 0)
                    completion_tokens = usage.get("completion_tokens", 0)
                    total_tokens = usage.get("total_tokens", 0)
                    
                    logger.info(f"âœ… LLM å“åº”æˆåŠŸ (è€—æ—¶: {elapsed:.2f}s)")
                    logger.info(f"   - Tokens: {prompt_tokens} (prompt) + {completion_tokens} (completion) = {total_tokens}")
                    logger.debug(f"   - å“åº”å†…å®¹: {result['choices'][0]['message']['content'][:200]}...")
                    
                    return result
                else:
                    logger.error(f"âŒ LLM API é”™è¯¯: {response.status_code}")
                    logger.error(f"   - å“åº”: {response.text}")
                    raise Exception(f"LLM API è¿”å›é”™è¯¯ {response.status_code}: {response.text}")
        
        except httpx.TimeoutException:
            elapsed = time.time() - start_time
            logger.error(f"âŒ LLM API è¯·æ±‚è¶…æ—¶ (å·²ç­‰å¾… {elapsed:.2f}s)")
            raise Exception("LLM API è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
        except httpx.ConnectError as e:
            logger.error(f"âŒ æ— æ³•è¿æ¥åˆ° LLM API: {e}")
            raise Exception(f"æ— æ³•è¿æ¥åˆ° API æœåŠ¡å™¨: {self.api_base}")
        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"âŒ LLM API è°ƒç”¨å¼‚å¸¸ (è€—æ—¶: {elapsed:.2f}s): {e}")
            raise
    
    async def review_chunk(
        self,
        text: str,
        relevant_rules: List[Dict[str, Any]],
        context: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        å®¡æ ¸æ–‡æœ¬å—
        
        Args:
            text: å¾…å®¡æ ¸æ–‡æœ¬
            relevant_rules: ç›¸å…³è§„åˆ™
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
        
        Returns:
            å®¡æ ¸ç»“æœï¼ˆJSON æ ¼å¼ï¼‰
        """
        # æ„é€  prompt
        prompt = self._build_review_prompt(text, relevant_rules, context)
        
        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å…¬æ–‡å®¡æ ¸åŠ©æ‰‹ï¼Œè´Ÿè´£æ£€æŸ¥æ–‡æ¡£æ˜¯å¦ç¬¦åˆå†™ä½œæ ‡å‡†ã€‚ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§æ ‡å‡†è¿›è¡Œå®¡æ ¸ï¼Œåªæ ‡æ³¨æ˜ç¡®è¿åæ ‡å‡†çš„åœ°æ–¹ã€‚"
            },
            {
                "role": "user",
                "content": prompt
            }
        ]
        
        # è°ƒç”¨ LLMï¼ˆè¦æ±‚ JSON æ ¼å¼è¾“å‡ºï¼‰
        response = await self.chat(
            messages=messages,
            temperature=0.1,  # ä½æ¸©åº¦ä¿è¯ä¸€è‡´æ€§
            response_format={"type": "json_object"}  # DeepSeek æ”¯æŒ
        )
        
        # è§£æå“åº”
        content = response["choices"][0]["message"]["content"]
        
        try:
            result = json.loads(content)
            return result
        except json.JSONDecodeError:
            logger.error(f"LLM è¿”å›çš„ä¸æ˜¯æœ‰æ•ˆ JSON: {content}")
            # å°è¯•æå– JSON
            import re
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            return {"issues": []}
    
    def _build_review_prompt(
        self,
        text: str,
        relevant_rules: List[Dict[str, Any]],
        context: Optional[str] = None
    ) -> str:
        """
        æ„é€ å®¡æ ¸ promptï¼ˆä¼˜åŒ–ç‰ˆ - æ›´ç²¾ç®€ï¼Œé€‚åˆå°æ¨¡å‹ï¼‰
        
        ä¼˜åŒ–ç‚¹ï¼š
        1. ç²¾ç®€è§„åˆ™æ•°é‡ï¼ˆåªå–æœ€ç›¸å…³çš„1-2æ¡ï¼‰
        2. ç®€åŒ–ç¤ºä¾‹ï¼ˆæ¯æ¡è§„åˆ™åªç»™1ä¸ªæ­£ä¾‹å’Œ1ä¸ªåä¾‹ï¼‰
        3. ä½¿ç”¨æ›´æ¸…æ™°çš„ç»“æ„åŒ–æ ¼å¼
        4. æ·»åŠ æ€ç»´é“¾å¼•å¯¼
        """
        # åªå–æœ€ç›¸å…³çš„è§„åˆ™ï¼ˆå‡å°‘ token æ¶ˆè€—ï¼‰
        top_rules = relevant_rules[:2] if len(relevant_rules) > 2 else relevant_rules
        
        # æ„å»ºç²¾ç®€çš„ prompt
        prompt = f"""ã€å®¡æ ¸ä»»åŠ¡ã€‘
æ£€æŸ¥ä»¥ä¸‹æ–‡æœ¬æ˜¯å¦è¿åå†™ä½œæ ‡å‡†ã€‚

ã€å¾…å®¡æ ¸æ–‡æœ¬ã€‘
{text}
"""
        
        # æ·»åŠ ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰
        if context:
            prompt += f"""
ã€ä¸Šä¸‹æ–‡ã€‘
{context}
"""
        
        # æ·»åŠ æ ‡å‡†ï¼ˆç²¾ç®€ç‰ˆï¼‰
        prompt += "\nã€é€‚ç”¨æ ‡å‡†ã€‘\n"
        for i, rule in enumerate(top_rules, 1):
            prompt += f"{i}. {rule.get('description', '')}\n"
            prompt += f"   è§„åˆ™ID: {rule.get('rule_id', '')} | ç±»åˆ«: {rule.get('category', '')} | ä¸¥é‡åº¦: {rule.get('severity', 'medium')}\n"
            
            # åªç»™1ä¸ªæ­£ä¾‹å’Œ1ä¸ªåä¾‹
            positive_examples = rule.get('positive_examples', [])
            negative_examples = rule.get('negative_examples', [])
            
            if positive_examples:
                prompt += f"   âœ… æ­£ç¡®: {positive_examples[0]}\n"
            if negative_examples:
                prompt += f"   âŒ é”™è¯¯: {negative_examples[0]}\n"
            prompt += "\n"
        
        # æ·»åŠ æ€ç»´é“¾å¼•å¯¼ï¼ˆå¸®åŠ©å°æ¨¡å‹æ›´å¥½åœ°æ¨ç†ï¼‰
        prompt += """ã€å®¡æ ¸æ­¥éª¤ã€‘
1. é€å¥é˜…è¯»æ–‡æœ¬
2. å¯¹æ¯”æ¯æ¡æ ‡å‡†
3. æ‰¾å‡ºæ˜ç¡®è¿åçš„åœ°æ–¹
4. å¦‚æœä¸ç¡®å®šï¼Œä¸è¦æ ‡æ³¨

ã€è¾“å‡ºæ ¼å¼ã€‘JSONæ ¼å¼ï¼Œç¤ºä¾‹ï¼š
{
  "issues": [
    {
      "position": "ç¬¬Xæ®µ",
      "rule_id": "R001",
      "category": "æ ‡é¢˜è§„èŒƒ",
      "original_text": "åŸæ–‡ç‰‡æ®µï¼ˆä¸è¶…è¿‡30å­—ï¼‰",
      "issue_description": "ä¸€å¥è¯è¯´æ˜é—®é¢˜",
      "suggestion": "ä¿®æ”¹å»ºè®®",
      "confidence": 0.9
    }
  ]
}

å¦‚æœæ²¡æœ‰é—®é¢˜ï¼Œè¿”å›ï¼š{"issues": []}

ç°åœ¨å¼€å§‹å®¡æ ¸ï¼Œåªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""
        
        return prompt

