"""
æ–‡æ¡£å®¡æ ¸å™¨ - æ ¸å¿ƒå®¡æ ¸å¼•æ“
"""
import asyncio
from typing import List, Dict, Any, Optional
from loguru import logger
from datetime import datetime
import uuid
from pathlib import Path

from ..models.document import DocumentChunk, Issue, ReviewResult, Severity
from ..services.llm_service import LLMService
from .document_parser import DocumentParser
from .chunker import SmartChunker
from .rag_engine import RAGEngine
from .review_logger import review_logger


class DocumentReviewer:
    """
    æ–‡æ¡£å®¡æ ¸å™¨
    
    æ ¸å¿ƒæµç¨‹ï¼š
    1. è§£ææ–‡æ¡£
    2. æ™ºèƒ½åˆ†å—
    3. RAG æ£€ç´¢ç›¸å…³è§„åˆ™
    4. LLM å®¡æ ¸
    5. ç»“æœèšåˆ
    """
    
    def __init__(
        self,
        rag_engine: RAGEngine,
        llm_service: LLMService,
        use_cross_chunk_check: bool = True
    ):
        self.parser = DocumentParser()
        self.chunker = SmartChunker()
        self.rag = rag_engine
        self.llm = llm_service
        self.use_cross_chunk_check = use_cross_chunk_check
    
    async def review_document(
        self,
        file_path: str,
        protocol_id: str,
        batch_size: int = 5
    ) -> ReviewResult:
        """
        å®¡æ ¸å®Œæ•´æ–‡æ¡£
        
        Args:
            file_path: æ–‡æ¡£è·¯å¾„
            protocol_id: ä½¿ç”¨çš„åè®®ID
            batch_size: æ‰¹é‡å¤„ç†å¤§å°
        
        Returns:
            å®¡æ ¸ç»“æœ
        """
        document_id = str(uuid.uuid4())
        document_name = Path(file_path).name
        logger.info(f"å¼€å§‹å®¡æ ¸æ–‡æ¡£: {file_path}, åè®®: {protocol_id}")
        
        # å¼€å§‹æ—¥å¿—ä¼šè¯
        session_id = review_logger.start_session(document_name, protocol_id)
        
        try:
            # 1. è§£ææ–‡æ¡£
            doc_structure = self.parser.parse_docx(file_path)
            
            # 2. æ™ºèƒ½åˆ†å—
            chunks = self.chunker.chunk_by_paragraphs(doc_structure)
            logger.info(f"æ–‡æ¡£åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªå—")
            
            # 3. æ‰¹é‡å®¡æ ¸
            all_issues = []
            total_chunks = len(chunks)
            
            logger.info(f"ğŸ“‹ å¼€å§‹é€å—å®¡æ ¸ï¼Œå…± {total_chunks} ä¸ªå—")
            logger.info(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
            
            for i in range(0, len(chunks), batch_size):
                batch = chunks[i:i+batch_size]
                batch_num = i // batch_size + 1
                total_batches = (total_chunks + batch_size - 1) // batch_size
                
                logger.info("=" * 60)
                logger.info(f"ğŸ“¦ æ‰¹æ¬¡ {batch_num}/{total_batches}: å®¡æ ¸å— {i+1}-{min(i+len(batch), total_chunks)}/{total_chunks}")
                logger.info("=" * 60)
                
                # å¹¶è¡Œå®¡æ ¸å½“å‰æ‰¹æ¬¡
                tasks = [
                    self._review_chunk(chunk, protocol_id)
                    for chunk in batch
                ]
                
                # ä½¿ç”¨ return_exceptions=False ç¡®ä¿å¼‚å¸¸ä¼šè¢«æŠ›å‡º
                try:
                    batch_results = await asyncio.gather(*tasks, return_exceptions=False)
                    
                    # æ”¶é›†ç»“æœ
                    batch_issues = 0
                    for issues in batch_results:
                        all_issues.extend(issues)
                        batch_issues += len(issues)
                    
                    logger.info(f"âœ… æ‰¹æ¬¡ {batch_num} å®Œæˆï¼Œå‘ç° {batch_issues} ä¸ªé—®é¢˜")
                    
                except Exception as e:
                    logger.error(f"âŒ æ‰¹æ¬¡ {batch_num} å®¡æ ¸å¤±è´¥: {e}")
                    # è®°å½•å¤±è´¥å¹¶ç»§ç»­æŠ›å‡º
                    raise Exception(f"å®¡æ ¸å¤±è´¥: {str(e)}ã€‚è¯·æ£€æŸ¥ï¼š1) API Key æ˜¯å¦æ­£ç¡® 2) ç½‘ç»œè¿æ¥ 3) æŸ¥çœ‹ç»ˆç«¯è¯¦ç»†æ—¥å¿—")
            
            # 4. è·¨æ®µè½äºŒæ¬¡æ£€æŸ¥ï¼ˆå¯é€‰ï¼‰
            if self.use_cross_chunk_check:
                cross_issues = await self._cross_chunk_check(chunks, protocol_id, all_issues)
                all_issues.extend(cross_issues)
            
            # 5. å»é‡å’Œæ’åº
            unique_issues = self._deduplicate_issues(all_issues)
            unique_issues.sort(key=lambda x: (x.page or 0, x.position))
            
            # 6. ç”Ÿæˆæ‘˜è¦
            summary = self._generate_summary(unique_issues)
            
            result = ReviewResult(
                document_id=document_id,
                protocol_id=protocol_id,
                total_issues=len(unique_issues),
                issues=unique_issues,
                summary=summary,
                created_at=datetime.now().isoformat()
            )
            
            logger.info(f"å®¡æ ¸å®Œæˆ: å‘ç° {len(unique_issues)} ä¸ªé—®é¢˜")
            
            # ç»“æŸæ—¥å¿—ä¼šè¯
            review_logger.end_session(result.dict())
            
            return result
        
        except Exception as e:
            logger.error(f"å®¡æ ¸å¤±è´¥: {e}")
            # å³ä½¿å¤±è´¥ä¹Ÿä¿å­˜æ—¥å¿—
            review_logger.end_session({"error": str(e)})
            raise
    
    async def _review_chunk(
        self,
        chunk: DocumentChunk,
        protocol_id: str
    ) -> List[Issue]:
        """
        å®¡æ ¸å•ä¸ªæ–‡æœ¬å—
        
        Args:
            chunk: æ–‡æ¡£å—
            protocol_id: åè®®ID
        
        Returns:
            é—®é¢˜åˆ—è¡¨
        """
        import time
        start_time = time.time()
        
        error_msg = None
        llm_prompt = ""
        llm_response = {}
        relevant_rules = []
        
        try:
            logger.info(f"ğŸ” å®¡æ ¸å—: {chunk.chunk_id}")
            logger.debug(f"   æ–‡æœ¬: {chunk.text[:100]}...")
            
            # 1. æ£€ç´¢ç›¸å…³è§„åˆ™
            relevant_rules = self.rag.retrieve_relevant_rules(
                text=chunk.text,
                protocol_id=protocol_id,
                top_k=3
            )
            
            if not relevant_rules:
                logger.debug(f"   âš ï¸  æ²¡æœ‰åŒ¹é…çš„è§„åˆ™ï¼Œè·³è¿‡")
                # è®°å½•æ—¥å¿—ï¼šæ²¡æœ‰åŒ¹é…è§„åˆ™
                review_logger.log_chunk_review(
                    chunk_id=chunk.chunk_id,
                    chunk_text=chunk.text,
                    relevant_rules=[],
                    llm_prompt="",
                    llm_response={"issues": [], "note": "æ²¡æœ‰åŒ¹é…çš„è§„åˆ™"},
                    issues_found=0
                )
                return []
            
            logger.info(f"   ğŸ“š åŒ¹é…åˆ° {len(relevant_rules)} æ¡è§„åˆ™")
            for rule in relevant_rules:
                logger.debug(f"      - {rule['rule_id']}: {rule['description'][:50]}...")
            
            # 2. æ„é€ ä¸Šä¸‹æ–‡
            context = None
            if chunk.context_before or chunk.context_after:
                context = f"å‰æ–‡: {chunk.context_before or 'æ— '}\nåæ–‡: {chunk.context_after or 'æ— '}"
            
            # 3. è°ƒç”¨ LLM å®¡æ ¸
            logger.info(f"   ğŸ¤– è°ƒç”¨ LLM è¿›è¡Œå®¡æ ¸...")
            
            # è·å– promptï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
            llm_prompt = self.llm._build_review_prompt(chunk.text, relevant_rules, context)
            
            result = await self.llm.review_chunk(
                text=chunk.text,
                relevant_rules=relevant_rules,
                context=context
            )
            llm_response = result
            
            elapsed = time.time() - start_time
            
            # 4. è§£æç»“æœ
            issues = []
            for item in result.get("issues", []):
                issue = Issue(
                    issue_id=str(uuid.uuid4()),
                    position=item.get("position", ""),
                    page=chunk.page,
                    rule_id=item.get("rule_id", ""),
                    category=item.get("category", ""),
                    original_text=item.get("original_text", ""),
                    issue_description=item.get("issue_description", ""),
                    suggestion=item.get("suggestion", ""),
                    confidence=item.get("confidence", 0.5),
                    severity=Severity(item.get("severity", "medium"))
                )
                
                # åªä¿ç•™é«˜ç½®ä¿¡åº¦çš„é—®é¢˜
                if issue.confidence >= 0.7:
                    issues.append(issue)
            
            if issues:
                logger.info(f"   âš ï¸  å‘ç° {len(issues)} ä¸ªé—®é¢˜ (è€—æ—¶: {elapsed:.2f}s)")
                for issue in issues:
                    logger.debug(f"      - [{issue.severity.value}] {issue.category}: {issue.issue_description[:50]}...")
            else:
                logger.info(f"   âœ… æœªå‘ç°é—®é¢˜ (è€—æ—¶: {elapsed:.2f}s)")
            
            # è®°å½•æˆåŠŸçš„å®¡æ ¸æ—¥å¿—
            review_logger.log_chunk_review(
                chunk_id=chunk.chunk_id,
                chunk_text=chunk.text,
                relevant_rules=relevant_rules,
                llm_prompt=llm_prompt,
                llm_response=llm_response,
                issues_found=len(issues)
            )
            
            return issues
        
        except Exception as e:
            elapsed = time.time() - start_time
            error_msg = str(e)
            logger.error(f"   âŒ å®¡æ ¸å¤±è´¥ (è€—æ—¶: {elapsed:.2f}s): {e}")
            import traceback
            error_detail = traceback.format_exc()
            logger.error(f"   è¯¦ç»†é”™è¯¯:\n{error_detail}")
            
            # è®°å½•å¤±è´¥çš„å®¡æ ¸æ—¥å¿—
            review_logger.log_chunk_review(
                chunk_id=chunk.chunk_id,
                chunk_text=chunk.text,
                relevant_rules=relevant_rules,
                llm_prompt=llm_prompt,
                llm_response=llm_response,
                issues_found=0,
                error=error_detail
            )
            
            # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚çŸ¥é“å‡ºé”™äº†
            raise
    
    async def _cross_chunk_check(
        self,
        chunks: List[DocumentChunk],
        protocol_id: str,
        existing_issues: List[Issue]
    ) -> List[Issue]:
        """
        è·¨æ®µè½æ£€æŸ¥
        
        è§£å†³è¯­ä¹‰æ–­è£‚é—®é¢˜çš„å…³é”®ï¼
        
        ç­–ç•¥ï¼š
        1. æ£€æµ‹å¯èƒ½çš„è·¨æ®µè½é—®é¢˜ï¼ˆå¦‚ï¼šå¼•ç”¨ã€é€»è¾‘è¿è´¯æ€§ï¼‰
        2. æ‰©å¤§ä¸Šä¸‹æ–‡çª—å£é‡æ–°å®¡æ ¸
        
        Args:
            chunks: æ‰€æœ‰æ–‡æ¡£å—
            protocol_id: åè®®ID
            existing_issues: å·²å‘ç°çš„é—®é¢˜
        
        Returns:
            æ–°å‘ç°çš„é—®é¢˜
        """
        logger.info("å¼€å§‹è·¨æ®µè½æ£€æŸ¥...")
        
        # è¯†åˆ«éœ€è¦è·¨æ®µè½æ£€æŸ¥çš„è§„åˆ™ç±»å‹
        cross_chunk_rules = self.rag.retrieve_relevant_rules(
            text="é€»è¾‘è¿è´¯æ€§ å¼•ç”¨å®Œæ•´æ€§ å‰åå‘¼åº”",
            protocol_id=protocol_id,
            top_k=5
        )
        
        if not cross_chunk_rules:
            return []
        
        new_issues = []
        
        # æ£€æŸ¥ç›¸é‚»æ®µè½
        for i in range(len(chunks) - 1):
            current = chunks[i]
            next_chunk = chunks[i + 1]
            
            # åˆå¹¶ç›¸é‚»æ®µè½
            combined_text = f"{current.text}\n{next_chunk.text}"
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è·¨æ®µè½é—®é¢˜
            try:
                result = await self.llm.review_chunk(
                    text=combined_text,
                    relevant_rules=cross_chunk_rules,
                    context=f"è¿™æ˜¯ç›¸é‚»çš„ä¸¤ä¸ªæ®µè½ï¼Œè¯·æ£€æŸ¥å®ƒä»¬ä¹‹é—´çš„é€»è¾‘è¿è´¯æ€§"
                )
                
                for item in result.get("issues", []):
                    # é¿å…é‡å¤
                    if not self._is_duplicate(item, existing_issues):
                        issue = Issue(
                            issue_id=str(uuid.uuid4()),
                            position=f"æ®µè½ {i} å’Œ {i+1} ä¹‹é—´",
                            page=current.page,
                            rule_id=item.get("rule_id", ""),
                            category=item.get("category", "è·¨æ®µè½é—®é¢˜"),
                            original_text=item.get("original_text", "")[:100],
                            issue_description=item.get("issue_description", ""),
                            suggestion=item.get("suggestion", ""),
                            confidence=item.get("confidence", 0.5),
                            severity=Severity(item.get("severity", "medium"))
                        )
                        
                        if issue.confidence >= 0.8:  # æ›´é«˜çš„é˜ˆå€¼
                            new_issues.append(issue)
            
            except Exception as e:
                logger.error(f"è·¨æ®µè½æ£€æŸ¥å¤±è´¥ ({i}, {i+1}): {e}")
        
        logger.info(f"è·¨æ®µè½æ£€æŸ¥å®Œæˆ: å‘ç° {len(new_issues)} ä¸ªæ–°é—®é¢˜")
        return new_issues
    
    def _deduplicate_issues(self, issues: List[Issue]) -> List[Issue]:
        """
        å»é‡
        
        Args:
            issues: é—®é¢˜åˆ—è¡¨
        
        Returns:
            å»é‡åçš„é—®é¢˜åˆ—è¡¨
        """
        seen = set()
        unique = []
        
        for issue in issues:
            # ä½¿ç”¨åŸæ–‡å’Œé—®é¢˜æè¿°ä½œä¸ºå”¯ä¸€æ ‡è¯†
            key = f"{issue.original_text[:50]}_{issue.issue_description[:50]}"
            
            if key not in seen:
                seen.add(key)
                unique.append(issue)
        
        return unique
    
    def _is_duplicate(self, new_item: Dict, existing_issues: List[Issue]) -> bool:
        """æ£€æŸ¥æ˜¯å¦é‡å¤"""
        new_text = new_item.get("original_text", "")[:50]
        
        for issue in existing_issues:
            if issue.original_text[:50] == new_text:
                return True
        
        return False
    
    def _generate_summary(self, issues: List[Issue]) -> Dict[str, Any]:
        """
        ç”Ÿæˆå®¡æ ¸æ‘˜è¦
        
        Args:
            issues: é—®é¢˜åˆ—è¡¨
        
        Returns:
            æ‘˜è¦ä¿¡æ¯
        """
        summary = {
            "total": len(issues),
            "by_severity": {
                "high": 0,
                "medium": 0,
                "low": 0
            },
            "by_category": {}
        }
        
        for issue in issues:
            # æŒ‰ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
            summary["by_severity"][issue.severity.value] += 1
            
            # æŒ‰ç±»åˆ«ç»Ÿè®¡
            category = issue.category
            if category not in summary["by_category"]:
                summary["by_category"][category] = 0
            summary["by_category"][category] += 1
        
        return summary

