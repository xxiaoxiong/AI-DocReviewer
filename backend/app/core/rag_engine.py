"""
RAG æ£€ç´¢å¼•æ“ - æ ‡å‡†çŸ¥è¯†åº“æ£€ç´¢
"""
import json
import numpy as np
from typing import List, Dict, Any, Optional
from pathlib import Path
from loguru import logger
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pickle

from ..models.document import Standard, Rule


class RAGEngine:
    """
    RAG æ£€ç´¢å¼•æ“
    
    åŠŸèƒ½ï¼š
    1. åŠ è½½æ ‡å‡†çŸ¥è¯†åº“
    2. å‘é‡åŒ–æ ‡å‡†è§„åˆ™
    3. æ ¹æ®æ–‡æœ¬æ£€ç´¢ç›¸å…³è§„åˆ™
    """
    
    def __init__(self, standards_dir: str = "standards/protocols"):
        self.standards_dir = Path(standards_dir)
        self.standards: Dict[str, Standard] = {}
        self.vectorizer = TfidfVectorizer(max_features=1000)
        self.rule_vectors = None
        self.rule_index = []  # è§„åˆ™ç´¢å¼•
        
        # åŠ è½½æ ‡å‡†
        self._load_standards()
        
        # æ„å»ºå‘é‡ç´¢å¼•
        self._build_vector_index()
    
    def _load_standards(self):
        """åŠ è½½æ‰€æœ‰æ ‡å‡†æ–‡ä»¶"""
        if not self.standards_dir.exists():
            logger.warning(f"æ ‡å‡†ç›®å½•ä¸å­˜åœ¨: {self.standards_dir}")
            return
        
        for file_path in self.standards_dir.glob("*.json"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    standard = Standard(**data)
                    self.standards[standard.protocol_id] = standard
                    logger.info(f"åŠ è½½æ ‡å‡†: {standard.name}")
            except Exception as e:
                logger.error(f"åŠ è½½æ ‡å‡†å¤±è´¥ {file_path}: {e}")
    
    def _build_vector_index(self):
        """
        æ„å»ºå‘é‡ç´¢å¼•
        
        å°†æ‰€æœ‰è§„åˆ™å‘é‡åŒ–ï¼Œç”¨äºå¿«é€Ÿæ£€ç´¢
        """
        if not self.standards:
            logger.warning("æ²¡æœ‰åŠ è½½ä»»ä½•æ ‡å‡†ï¼Œè·³è¿‡å‘é‡ç´¢å¼•æ„å»º")
            return
        
        # æ”¶é›†æ‰€æœ‰è§„åˆ™
        all_rules = []
        for standard in self.standards.values():
            for category in standard.categories:
                for rule in category.rules:
                    all_rules.append({
                        "protocol_id": standard.protocol_id,
                        "category": category.category,
                        "rule": rule
                    })
        
        if not all_rules:
            return
        
        # æ„å»ºæ–‡æœ¬è¯­æ–™ï¼ˆç”¨äºå‘é‡åŒ–ï¼‰
        corpus = []
        for item in all_rules:
            rule = item["rule"]
            # ç»„åˆè§„åˆ™çš„å¤šä¸ªå­—æ®µ
            text = f"{rule.description} {' '.join(rule.keywords)} {' '.join(rule.positive_examples[:2])}"
            corpus.append(text)
            self.rule_index.append(item)
        
        # å‘é‡åŒ–
        self.rule_vectors = self.vectorizer.fit_transform(corpus)
        
        logger.info(f"å‘é‡ç´¢å¼•æ„å»ºå®Œæˆ: {len(self.rule_index)} æ¡è§„åˆ™")
    
    def retrieve_relevant_rules(
        self,
        text: str,
        protocol_id: Optional[str] = None,
        top_k: int = 3
    ) -> List[Dict[str, Any]]:
        """
        æ£€ç´¢ç›¸å…³è§„åˆ™
        
        Args:
            text: å¾…æ£€ç´¢æ–‡æœ¬
            protocol_id: æŒ‡å®šåè®®IDï¼ˆå¦‚æœä¸ºç©ºåˆ™æ£€ç´¢æ‰€æœ‰ï¼‰
            top_k: è¿”å›å‰ k ä¸ªæœ€ç›¸å…³çš„è§„åˆ™
        
        Returns:
            ç›¸å…³è§„åˆ™åˆ—è¡¨
        """
        if self.rule_vectors is None or not self.rule_index:
            logger.warning("âŒ å‘é‡ç´¢å¼•æœªæ„å»ºï¼Œè¿”å›ç©ºç»“æœ")
            return []
        
        logger.debug(f"ğŸ” RAG æ£€ç´¢: æ–‡æœ¬='{text[:50]}...', åè®®={protocol_id}")
        
        # å¦‚æœæŒ‡å®šäº†åè®®ï¼Œå…ˆè¿‡æ»¤å‡ºè¯¥åè®®çš„è§„åˆ™ç´¢å¼•
        if protocol_id:
            protocol_indices = [
                i for i, item in enumerate(self.rule_index)
                if item["protocol_id"] == protocol_id
            ]
            
            if not protocol_indices:
                logger.warning(f"âŒ åè®® {protocol_id} æ²¡æœ‰ä»»ä½•è§„åˆ™")
                return []
            
            logger.debug(f"   åè®® {protocol_id} å…±æœ‰ {len(protocol_indices)} æ¡è§„åˆ™")
            
            # åªå¯¹è¯¥åè®®çš„è§„åˆ™è®¡ç®—ç›¸ä¼¼åº¦
            protocol_vectors = self.rule_vectors[protocol_indices]
            query_vector = self.vectorizer.transform([text])
            similarities = cosine_similarity(query_vector, protocol_vectors)[0]
            
            # è·å– top-k
            if len(similarities) < top_k:
                top_k = len(similarities)
            
            top_local_indices = np.argsort(similarities)[-top_k:][::-1]
            top_indices = [protocol_indices[i] for i in top_local_indices]
            top_similarities = similarities[top_local_indices]
        else:
            # æ£€ç´¢æ‰€æœ‰è§„åˆ™
            query_vector = self.vectorizer.transform([text])
            similarities = cosine_similarity(query_vector, self.rule_vectors)[0]
            top_indices = np.argsort(similarities)[-top_k:][::-1]
            top_similarities = similarities[top_indices]
        
        # æ„é€ ç»“æœï¼ˆé™ä½ç›¸ä¼¼åº¦é˜ˆå€¼åˆ° 0.01ï¼Œå‡ ä¹ä¸è¿‡æ»¤ï¼‰
        results = []
        for idx, similarity in zip(top_indices, top_similarities):
            item = self.rule_index[idx]
            rule = item["rule"]
            
            results.append({
                "rule_id": rule.rule_id,
                "category": item["category"],
                "description": rule.description,
                "check_type": rule.check_type,
                "keywords": rule.keywords,
                "positive_examples": rule.positive_examples,
                "negative_examples": rule.negative_examples,
                "severity": rule.severity,
                "similarity": float(similarity)
            })
            
            logger.debug(f"   âœ… è§„åˆ™ {rule.rule_id}: {rule.description[:30]}... (ç›¸ä¼¼åº¦: {similarity:.3f})")
        
        if not results:
            logger.warning(f"âŒ æ²¡æœ‰æ£€ç´¢åˆ°ä»»ä½•è§„åˆ™ï¼")
        else:
            logger.info(f"âœ… æ£€ç´¢åˆ° {len(results)} æ¡ç›¸å…³è§„åˆ™")
        
        return results
    
    def get_all_rules_by_protocol(self, protocol_id: str) -> List[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šåè®®çš„æ‰€æœ‰è§„åˆ™
        
        Args:
            protocol_id: åè®®ID
        
        Returns:
            è§„åˆ™åˆ—è¡¨
        """
        if protocol_id not in self.standards:
            logger.warning(f"åè®®ä¸å­˜åœ¨: {protocol_id}")
            return []
        
        standard = self.standards[protocol_id]
        rules = []
        
        for category in standard.categories:
            for rule in category.rules:
                rules.append({
                    "rule_id": rule.rule_id,
                    "category": category.category,
                    "description": rule.description,
                    "check_type": rule.check_type,
                    "keywords": rule.keywords,
                    "positive_examples": rule.positive_examples,
                    "negative_examples": rule.negative_examples,
                    "severity": rule.severity
                })
        
        return rules
    
    def list_available_protocols(self) -> List[Dict[str, str]]:
        """
        åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„åè®®
        
        Returns:
            åè®®åˆ—è¡¨
        """
        return [
            {
                "protocol_id": std.protocol_id,
                "name": std.name,
                "version": std.version,
                "description": std.description or ""
            }
            for std in self.standards.values()
        ]
    
    def save_index(self, file_path: str = "standards/embeddings/index.pkl"):
        """ä¿å­˜å‘é‡ç´¢å¼•ï¼ˆå¯é€‰ï¼Œç”¨äºåŠ é€Ÿå¯åŠ¨ï¼‰"""
        try:
            with open(file_path, 'wb') as f:
                pickle.dump({
                    "vectorizer": self.vectorizer,
                    "rule_vectors": self.rule_vectors,
                    "rule_index": self.rule_index
                }, f)
            logger.info(f"å‘é‡ç´¢å¼•å·²ä¿å­˜: {file_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜å‘é‡ç´¢å¼•å¤±è´¥: {e}")
    
    def load_index(self, file_path: str = "standards/embeddings/index.pkl"):
        """åŠ è½½å‘é‡ç´¢å¼•"""
        try:
            with open(file_path, 'rb') as f:
                data = pickle.load(f)
                self.vectorizer = data["vectorizer"]
                self.rule_vectors = data["rule_vectors"]
                self.rule_index = data["rule_index"]
            logger.info(f"å‘é‡ç´¢å¼•å·²åŠ è½½: {file_path}")
        except Exception as e:
            logger.error(f"åŠ è½½å‘é‡ç´¢å¼•å¤±è´¥: {e}")

