"""
å®¡æ ¸æ—¥å¿—è®°å½•å™¨ - è®°å½•æ‰€æœ‰ LLM è°ƒç”¨å’Œç»“æœ
"""
import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional
from loguru import logger


class ReviewLogger:
    """å®¡æ ¸æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, log_dir: str = "logs/reviews"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.current_session = None
        self.session_logs = []
    
    def start_session(self, document_name: str, protocol_id: str) -> str:
        """
        å¼€å§‹æ–°çš„å®¡æ ¸ä¼šè¯
        
        Args:
            document_name: æ–‡æ¡£åç§°
            protocol_id: åè®®ID
        
        Returns:
            ä¼šè¯ID
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        session_id = f"{timestamp}_{document_name.replace('.', '_')}"
        
        self.current_session = {
            "session_id": session_id,
            "document_name": document_name,
            "protocol_id": protocol_id,
            "start_time": datetime.now().isoformat(),
            "chunks": [],
            "total_llm_calls": 0,
            "total_issues_found": 0
        }
        
        self.session_logs = []
        logger.info(f"ğŸ“ å¼€å§‹å®¡æ ¸ä¼šè¯: {session_id}")
        return session_id
    
    def log_chunk_review(
        self,
        chunk_id: str,
        chunk_text: str,
        relevant_rules: List[Dict[str, Any]],
        llm_prompt: str,
        llm_response: Dict[str, Any],
        issues_found: int,
        error: Optional[str] = None
    ):
        """
        è®°å½•å•ä¸ªå—çš„å®¡æ ¸è¿‡ç¨‹
        
        Args:
            chunk_id: å—ID
            chunk_text: å—æ–‡æœ¬
            relevant_rules: ç›¸å…³è§„åˆ™
            llm_prompt: å‘é€ç»™ LLM çš„ prompt
            llm_response: LLM è¿”å›çš„å“åº”
            issues_found: å‘ç°çš„é—®é¢˜æ•°
            error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        """
        log_entry = {
            "chunk_id": chunk_id,
            "timestamp": datetime.now().isoformat(),
            "chunk_text": chunk_text[:200] + "..." if len(chunk_text) > 200 else chunk_text,
            "chunk_length": len(chunk_text),
            "relevant_rules_count": len(relevant_rules),
            "relevant_rules": [
                {
                    "rule_id": r.get("rule_id"),
                    "category": r.get("category"),
                    "description": r.get("description")
                }
                for r in relevant_rules
            ],
            "llm_prompt_length": len(llm_prompt),
            "llm_prompt": llm_prompt,  # å®Œæ•´ prompt
            "llm_response": llm_response,  # å®Œæ•´å“åº”
            "issues_found": issues_found,
            "error": error,
            "success": error is None
        }
        
        self.session_logs.append(log_entry)
        
        if self.current_session:
            self.current_session["chunks"].append(log_entry)
            self.current_session["total_llm_calls"] += 1
            self.current_session["total_issues_found"] += issues_found
        
        # å®æ—¶ä¿å­˜ï¼ˆé˜²æ­¢å´©æºƒä¸¢å¤±æ•°æ®ï¼‰
        self._save_current_chunk(log_entry)
        
        logger.info(f"ğŸ“Š å— {chunk_id}: è°ƒç”¨ LLM âœ…, å‘ç° {issues_found} ä¸ªé—®é¢˜")
    
    def end_session(self, final_result: Dict[str, Any]):
        """
        ç»“æŸå®¡æ ¸ä¼šè¯å¹¶ä¿å­˜å®Œæ•´æ—¥å¿—
        
        Args:
            final_result: æœ€ç»ˆå®¡æ ¸ç»“æœ
        """
        if not self.current_session:
            logger.warning("æ²¡æœ‰æ´»åŠ¨çš„å®¡æ ¸ä¼šè¯")
            return
        
        self.current_session["end_time"] = datetime.now().isoformat()
        self.current_session["final_result"] = final_result
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        self.current_session["statistics"] = self._calculate_statistics()
        
        # ä¿å­˜å®Œæ•´æ—¥å¿—
        self._save_session_log()
        
        logger.info(f"âœ… å®¡æ ¸ä¼šè¯ç»“æŸ: {self.current_session['session_id']}")
        logger.info(f"   - æ€»è°ƒç”¨æ¬¡æ•°: {self.current_session['total_llm_calls']}")
        logger.info(f"   - å‘ç°é—®é¢˜æ•°: {self.current_session['total_issues_found']}")
        
        self.current_session = None
        self.session_logs = []
    
    def _calculate_statistics(self) -> Dict[str, Any]:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        if not self.session_logs:
            return {}
        
        total_chunks = len(self.session_logs)
        successful_calls = sum(1 for log in self.session_logs if log["success"])
        failed_calls = total_chunks - successful_calls
        
        total_issues = sum(log["issues_found"] for log in self.session_logs)
        chunks_with_issues = sum(1 for log in self.session_logs if log["issues_found"] > 0)
        
        return {
            "total_chunks": total_chunks,
            "successful_llm_calls": successful_calls,
            "failed_llm_calls": failed_calls,
            "success_rate": f"{successful_calls / total_chunks * 100:.1f}%",
            "total_issues_found": total_issues,
            "chunks_with_issues": chunks_with_issues,
            "average_issues_per_chunk": f"{total_issues / total_chunks:.2f}",
            "issue_detection_rate": f"{chunks_with_issues / total_chunks * 100:.1f}%"
        }
    
    def _save_current_chunk(self, log_entry: Dict[str, Any]):
        """å®æ—¶ä¿å­˜å½“å‰å—çš„æ—¥å¿—"""
        if not self.current_session:
            return
        
        session_id = self.current_session["session_id"]
        chunk_log_file = self.log_dir / f"{session_id}_chunks.jsonl"
        
        try:
            with open(chunk_log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
        except Exception as e:
            logger.error(f"ä¿å­˜å—æ—¥å¿—å¤±è´¥: {e}")
    
    def _save_session_log(self):
        """ä¿å­˜å®Œæ•´çš„ä¼šè¯æ—¥å¿—"""
        if not self.current_session:
            return
        
        session_id = self.current_session["session_id"]
        
        # ä¿å­˜å®Œæ•´æ—¥å¿—ï¼ˆJSON æ ¼å¼ï¼Œä¾¿äºæŸ¥çœ‹ï¼‰
        full_log_file = self.log_dir / f"{session_id}_full.json"
        try:
            with open(full_log_file, 'w', encoding='utf-8') as f:
                json.dump(self.current_session, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ å®Œæ•´æ—¥å¿—å·²ä¿å­˜: {full_log_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜å®Œæ•´æ—¥å¿—å¤±è´¥: {e}")
        
        # ä¿å­˜æ‘˜è¦ï¼ˆä¾¿äºå¿«é€ŸæŸ¥çœ‹ï¼‰
        summary_file = self.log_dir / f"{session_id}_summary.txt"
        try:
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write("=" * 80 + "\n")
                f.write(f"å®¡æ ¸ä¼šè¯æ‘˜è¦: {session_id}\n")
                f.write("=" * 80 + "\n\n")
                
                f.write(f"æ–‡æ¡£åç§°: {self.current_session['document_name']}\n")
                f.write(f"ä½¿ç”¨åè®®: {self.current_session['protocol_id']}\n")
                f.write(f"å¼€å§‹æ—¶é—´: {self.current_session['start_time']}\n")
                f.write(f"ç»“æŸæ—¶é—´: {self.current_session['end_time']}\n\n")
                
                stats = self.current_session['statistics']
                f.write("ç»Ÿè®¡ä¿¡æ¯:\n")
                f.write(f"  - æ€»å—æ•°: {stats['total_chunks']}\n")
                f.write(f"  - æˆåŠŸè°ƒç”¨: {stats['successful_llm_calls']}\n")
                f.write(f"  - å¤±è´¥è°ƒç”¨: {stats['failed_llm_calls']}\n")
                f.write(f"  - æˆåŠŸç‡: {stats['success_rate']}\n")
                f.write(f"  - å‘ç°é—®é¢˜æ•°: {stats['total_issues_found']}\n")
                f.write(f"  - æœ‰é—®é¢˜çš„å—: {stats['chunks_with_issues']}\n")
                f.write(f"  - å¹³å‡æ¯å—é—®é¢˜æ•°: {stats['average_issues_per_chunk']}\n")
                f.write(f"  - é—®é¢˜æ£€å‡ºç‡: {stats['issue_detection_rate']}\n\n")
                
                f.write("=" * 80 + "\n")
                f.write("è¯¦ç»†ç»“æœ\n")
                f.write("=" * 80 + "\n\n")
                
                for i, chunk_log in enumerate(self.session_logs, 1):
                    f.write(f"å— {i}: {chunk_log['chunk_id']}\n")
                    f.write(f"  æ–‡æœ¬: {chunk_log['chunk_text']}\n")
                    f.write(f"  è§„åˆ™æ•°: {chunk_log['relevant_rules_count']}\n")
                    f.write(f"  å‘ç°é—®é¢˜: {chunk_log['issues_found']}\n")
                    if chunk_log['error']:
                        f.write(f"  é”™è¯¯: {chunk_log['error']}\n")
                    f.write("\n")
            
            logger.info(f"ğŸ“„ æ‘˜è¦å·²ä¿å­˜: {summary_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜æ‘˜è¦å¤±è´¥: {e}")
    
    def get_recent_sessions(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        è·å–æœ€è¿‘çš„å®¡æ ¸ä¼šè¯
        
        Args:
            limit: è¿”å›æ•°é‡
        
        Returns:
            ä¼šè¯åˆ—è¡¨
        """
        summary_files = sorted(
            self.log_dir.glob("*_summary.txt"),
            key=lambda x: x.stat().st_mtime,
            reverse=True
        )[:limit]
        
        sessions = []
        for file in summary_files:
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    sessions.append({
                        "file": file.name,
                        "path": str(file),
                        "modified": datetime.fromtimestamp(file.stat().st_mtime).isoformat()
                    })
            except Exception as e:
                logger.error(f"è¯»å–ä¼šè¯æ–‡ä»¶å¤±è´¥ {file}: {e}")
        
        return sessions


# å…¨å±€å®ä¾‹
review_logger = ReviewLogger()

