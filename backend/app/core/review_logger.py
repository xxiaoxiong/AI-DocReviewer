"""
å®¡æ ¸æ—¥å¿—è®°å½•å™¨ - è®°å½•æ‰€æœ‰ LLM è°ƒç”¨å’Œç»“æœ
"""
import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional
from loguru import logger


class ReviewLogger:
    """å®¡æ ¸æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, log_dir: str = "logs/reviews"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.current_session = None
        self.session_logs = []
    
    def log_chunk_review(
        self,
        chunk_id: str,
        chunk_text: str,
        relevant_rules: List[Dict[str, Any]],
        llm_prompt: str,
        llm_response: Dict[str, Any],
        issues_found: int,
        error: Optional[str] = None
    ):
        """
        è®°å½•å•ä¸ªå—çš„å®¡æ ¸è¿‡ç¨‹
        
        Args:
            chunk_id: å—ID
            chunk_text: å—æ–‡æœ¬
            relevant_rules: ç›¸å…³è§„åˆ™
            llm_prompt: å‘é€ç»™ LLM çš„ prompt
            llm_response: LLM è¿”å›çš„å“åº”
            issues_found: å‘ç°çš„é—®é¢˜æ•°
            error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        """
        log_entry = {
            "chunk_id": chunk_id,
            "timestamp": datetime.now().isoformat(),
            "chunk_text": chunk_text[:200] + "..." if len(chunk_text) > 200 else chunk_text,
            "chunk_length": len(chunk_text),
            "relevant_rules_count": len(relevant_rules),
            "relevant_rules": [
                {
                    "rule_id": r.get("rule_id"),
                    "category": r.get("category"),
                    "description": r.get("description")
                }
                for r in relevant_rules
            ],
            "llm_prompt_length": len(llm_prompt),
            "llm_prompt": llm_prompt,  # å®Œæ•´ prompt
            "llm_response": llm_response,  # å®Œæ•´å“åº”
            "issues_found": issues_found,
            "error": error,
            "success": error is None
        }
        
        self.session_logs.append(log_entry)
        
        if self.current_session:
            self.current_session["chunks"].append(log_entry)
            self.current_session["total_llm_calls"] += 1
            self.current_session["total_issues_found"] += issues_found
        
        # å®æ—¶ä¿å­˜ï¼ˆé˜²æ­¢å´©æºƒä¸¢å¤±æ•°æ®ï¼‰
        self._save_current_chunk(log_entry)
        
        logger.info(f"ğŸ“Š å— {chunk_id}: è°ƒç”¨ LLM âœ…, å‘ç° {issues_found} ä¸ªé—®é¢˜")
    
    def _save_current_chunk(self, log_entry: Dict[str, Any]):
        """å®æ—¶ä¿å­˜å½“å‰å—çš„æ—¥å¿—"""
        if not self.current_session:
            return
        
        session_id = self.current_session["session_id"]
        chunk_log_file = self.log_dir / f"{session_id}_chunks.jsonl"
        
        try:
            with open(chunk_log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
        except Exception as e:
            logger.error(f"ä¿å­˜å—æ—¥å¿—å¤±è´¥: {e}")
    
    def get_recent_sessions(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        è·å–æœ€è¿‘çš„å®¡æ ¸ä¼šè¯
        
        Args:
            limit: è¿”å›æ•°é‡
        
        Returns:
            ä¼šè¯åˆ—è¡¨
        """
        summary_files = sorted(
            self.log_dir.glob("*_summary.txt"),
            key=lambda x: x.stat().st_mtime,
            reverse=True
        )[:limit]
        
        sessions = []
        for file in summary_files:
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    sessions.append({
                        "file": file.name,
                        "path": str(file),
                        "modified": datetime.fromtimestamp(file.stat().st_mtime).isoformat()
                    })
            except Exception as e:
                logger.error(f"è¯»å–ä¼šè¯æ–‡ä»¶å¤±è´¥ {file}: {e}")
        
        return sessions


# å…¨å±€å®ä¾‹
review_logger = ReviewLogger()

