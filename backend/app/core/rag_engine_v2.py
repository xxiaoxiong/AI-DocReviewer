"""
RAG æ£€ç´¢å¼•æ“ V2 - ä½¿ç”¨è¯­ä¹‰åµŒå…¥æ¨¡å‹ï¼ˆBGEï¼‰

å‡çº§ç‚¹ï¼š
1. TF-IDF â†’ æ·±åº¦å­¦ä¹ è¯­ä¹‰åµŒå…¥
2. ç†è§£è¯­ä¹‰ï¼Œæ”¯æŒåŒä¹‰è¯
3. ä¸­æ–‡å‹å¥½ï¼Œæ£€ç´¢æ›´å‡†ç¡®
"""
import json
import numpy as np
from typing import List, Dict, Any, Optional
from pathlib import Path
from loguru import logger
import pickle
import faiss

from ..models.document import Standard, Rule


class RAGEngineV2:
    """
    RAG æ£€ç´¢å¼•æ“ V2 - è¯­ä¹‰æ£€ç´¢ç‰ˆæœ¬
    
    åŠŸèƒ½ï¼š
    1. åŠ è½½æ ‡å‡†çŸ¥è¯†åº“
    2. ä½¿ç”¨è¯­ä¹‰åµŒå…¥æ¨¡å‹å‘é‡åŒ–è§„åˆ™ï¼ˆæ”¯æŒåƒé—®3/BGEï¼‰
    3. ä½¿ç”¨ FAISS è¿›è¡Œé«˜æ•ˆå‘é‡æ£€ç´¢
    4. æ··åˆæ£€ç´¢ï¼ˆè¯­ä¹‰+å…³é”®è¯ï¼‰
    
    æ¨èæ¨¡å‹ï¼š
    - BAAI/bge-small-zh-v1.5 (BGE) - è½»é‡çº§ï¼Œæ¼”ç¤ºç‰ˆæœ¬æ¨è
      * æ¨¡å‹å°ï¼ˆ~100MBï¼‰ï¼Œå¯åŠ¨å¿«
      * é€‚åˆèµ„æºå—é™ç¯å¢ƒ
    
    """
    
    def __init__(
        self, 
        standards_dir: str = "standards/protocols",
        model_name: str = "BAAI/bge-small-zh-v1.5",  # BGE è½»é‡çº§æ¨¡å‹ï¼ˆæ¼”ç¤ºç‰ˆæœ¬ï¼‰
        # model_name: str = "Alibaba-NLP/gte-Qwen2-1.5B-instruct",  # åƒé—®3ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
        use_faiss: bool = True
    ):
        self.standards_dir = Path(standards_dir)
        self.standards: Dict[str, Standard] = {}
        self.model_name = model_name
        self.use_faiss = use_faiss
        
        # å»¶è¿ŸåŠ è½½æ¨¡å‹ï¼ˆé¿å…å¯åŠ¨æ—¶åŠ è½½ï¼‰
        self.model = None
        self.rule_vectors = None
        self.rule_index = []  # è§„åˆ™ç´¢å¼•
        self.faiss_index = None
        
        # åŠ è½½æ ‡å‡†
        self._load_standards()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_model()
        
        # æ„å»ºå‘é‡ç´¢å¼•
        self._build_vector_index()
    
    def _init_model(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        try:
            import os
            from sentence_transformers import SentenceTransformer
            
            # è®¾ç½®å›½å†…é•œåƒæºï¼ˆè§£å†³ç½‘ç»œé—®é¢˜ï¼‰
            os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
            
            logger.info(f"ğŸ¤– åŠ è½½è¯­ä¹‰åµŒå…¥æ¨¡å‹: {self.model_name}")
            
            # æ ¹æ®æ¨¡å‹ç»™å‡ºä¸‹è½½æç¤º
            if "qwen" in self.model_name.lower():
                logger.info("   é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½åƒé—®3æ¨¡å‹ï¼ˆçº¦ 3GBï¼‰ï¼Œè¯·ç¨å€™...")
                logger.info("   ğŸ’¡ åƒé—®3æ”¯æŒè¶…é•¿æ–‡æœ¬ï¼ˆ8192 tokensï¼‰ï¼Œæ£€ç´¢æ›´å‡†ç¡®")
            else:
                logger.info("   é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆçº¦ 100-400MBï¼‰ï¼Œè¯·ç¨å€™...")
            
            logger.info("   ä½¿ç”¨é•œåƒæº: https://hf-mirror.com")
            
            self.model = SentenceTransformer(self.model_name)
            
            logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
            logger.info(f"   æ¨¡å‹ç»´åº¦: {self.model.get_sentence_embedding_dimension()}")
            
        except ImportError:
            logger.error("âŒ æœªå®‰è£… sentence-transformersï¼Œè¯·è¿è¡Œï¼š")
            logger.error("   pip install sentence-transformers torch")
            raise
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            logger.error("   å¦‚æœæ˜¯ç½‘ç»œé—®é¢˜ï¼Œå¯ä»¥æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°")
            raise
    
    def _load_standards(self):
        """åŠ è½½æ‰€æœ‰æ ‡å‡†æ–‡ä»¶"""
        if not self.standards_dir.exists():
            logger.warning(f"æ ‡å‡†ç›®å½•ä¸å­˜åœ¨: {self.standards_dir}")
            return
        
        for file_path in self.standards_dir.glob("*.json"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    standard = Standard(**data)
                    self.standards[standard.protocol_id] = standard
                    logger.info(f"åŠ è½½æ ‡å‡†: {standard.name}")
            except Exception as e:
                logger.error(f"åŠ è½½æ ‡å‡†å¤±è´¥ {file_path}: {e}")
    
    def _build_vector_index(self):
        """
        æ„å»ºå‘é‡ç´¢å¼•ï¼ˆä½¿ç”¨è¯­ä¹‰åµŒå…¥ï¼‰
        
        å°†æ‰€æœ‰è§„åˆ™å‘é‡åŒ–ï¼Œç”¨äºå¿«é€Ÿæ£€ç´¢
        """
        if not self.standards:
            logger.warning("æ²¡æœ‰åŠ è½½ä»»ä½•æ ‡å‡†ï¼Œè·³è¿‡å‘é‡ç´¢å¼•æ„å»º")
            return
        
        if self.model is None:
            logger.error("æ¨¡å‹æœªåˆå§‹åŒ–")
            return
        
        logger.info("ğŸ”¨ å¼€å§‹æ„å»ºè¯­ä¹‰å‘é‡ç´¢å¼•...")
        
        # æ”¶é›†æ‰€æœ‰è§„åˆ™
        all_rules = []
        for standard in self.standards.values():
            for category in standard.categories:
                for rule in category.rules:
                    all_rules.append({
                        "protocol_id": standard.protocol_id,
                        "category": category.category,
                        "rule": rule
                    })
        
        if not all_rules:
            return
        
        # æ„å»ºæ–‡æœ¬è¯­æ–™ï¼ˆç”¨äºå‘é‡åŒ–ï¼‰
        corpus = []
        for item in all_rules:
            rule = item["rule"]
            # ç»„åˆè§„åˆ™çš„å¤šä¸ªå­—æ®µï¼ˆæ›´ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼‰
            text = f"{rule.description} {' '.join(rule.keywords)} {' '.join(rule.positive_examples[:2])}"
            corpus.append(text)
            self.rule_index.append(item)
        
        # ä½¿ç”¨ BGE æ¨¡å‹è¿›è¡Œå‘é‡åŒ–ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
        logger.info(f"   æ­£åœ¨å‘é‡åŒ– {len(corpus)} æ¡è§„åˆ™...")
        self.rule_vectors = self.model.encode(
            corpus,
            batch_size=32,
            show_progress_bar=True,
            normalize_embeddings=True  # å½’ä¸€åŒ–ï¼Œä¾¿äºè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        )
        
        # æ„å»º FAISS ç´¢å¼•ï¼ˆå¯é€‰ï¼Œç”¨äºå¤§è§„æ¨¡æ£€ç´¢åŠ é€Ÿï¼‰
        if self.use_faiss:
            dimension = self.rule_vectors.shape[1]
            self.faiss_index = faiss.IndexFlatIP(dimension)  # å†…ç§¯ç´¢å¼•ï¼ˆå½’ä¸€åŒ–åç­‰ä»·äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
            self.faiss_index.add(self.rule_vectors.astype('float32'))
            logger.info(f"âœ… FAISS ç´¢å¼•æ„å»ºå®Œæˆ")
        
        logger.info(f"âœ… è¯­ä¹‰å‘é‡ç´¢å¼•æ„å»ºå®Œæˆ: {len(self.rule_index)} æ¡è§„åˆ™")
    
    def retrieve_relevant_rules(
        self,
        text: str,
        protocol_id: Optional[str] = None,
        top_k: int = 3,
        use_hybrid: bool = True,
        min_similarity: float = 0.3
    ) -> List[Dict[str, Any]]:
        """
        æ£€ç´¢ç›¸å…³è§„åˆ™ï¼ˆæ··åˆæ£€ç´¢ï¼šè¯­ä¹‰ + å…³é”®è¯ï¼‰
        
        Args:
            text: å¾…æ£€ç´¢æ–‡æœ¬
            protocol_id: æŒ‡å®šåè®®IDï¼ˆå¦‚æœä¸ºç©ºåˆ™æ£€ç´¢æ‰€æœ‰ï¼‰
            top_k: è¿”å›å‰ k ä¸ªæœ€ç›¸å…³çš„è§„åˆ™
            use_hybrid: æ˜¯å¦ä½¿ç”¨æ··åˆæ£€ç´¢ï¼ˆè¯­ä¹‰+å…³é”®è¯ï¼‰
            min_similarity: æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
        
        Returns:
            ç›¸å…³è§„åˆ™åˆ—è¡¨
        """
        if self.rule_vectors is None or not self.rule_index:
            logger.warning("âŒ å‘é‡ç´¢å¼•æœªæ„å»ºï¼Œè¿”å›ç©ºç»“æœ")
            return []
        
        logger.debug(f"ğŸ” {'æ··åˆ' if use_hybrid else 'è¯­ä¹‰'}æ£€ç´¢: æ–‡æœ¬='{text[:50]}...', åè®®={protocol_id}")
        
        # å‘é‡åŒ–æŸ¥è¯¢æ–‡æœ¬
        query_vector = self.model.encode(
            [text],
            normalize_embeddings=True
        )[0]
        
        # å¦‚æœæŒ‡å®šäº†åè®®ï¼Œå…ˆè¿‡æ»¤
        if protocol_id:
            protocol_indices = [
                i for i, item in enumerate(self.rule_index)
                if item["protocol_id"] == protocol_id
            ]
            
            if not protocol_indices:
                logger.warning(f"âŒ åè®® {protocol_id} æ²¡æœ‰ä»»ä½•è§„åˆ™")
                return []
            
            logger.debug(f"   åè®® {protocol_id} å…±æœ‰ {len(protocol_indices)} æ¡è§„åˆ™")
            
            # åªå¯¹è¯¥åè®®çš„è§„åˆ™è®¡ç®—ç›¸ä¼¼åº¦
            protocol_vectors = self.rule_vectors[protocol_indices]
            semantic_similarities = np.dot(protocol_vectors, query_vector)  # ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå·²å½’ä¸€åŒ–ï¼‰
            
            # æ··åˆæ£€ç´¢ï¼šç»“åˆå…³é”®è¯åŒ¹é…
            if use_hybrid:
                keyword_scores = np.array([
                    self._keyword_match_score(text, self.rule_index[idx]["rule"])
                    for idx in protocol_indices
                ])
                
                # èåˆåˆ†æ•°ï¼ˆè¯­ä¹‰ 70% + å…³é”®è¯ 30%ï¼‰
                final_scores = 0.7 * semantic_similarities + 0.3 * keyword_scores
                logger.debug(f"   ä½¿ç”¨æ··åˆæ£€ç´¢ï¼ˆè¯­ä¹‰ 70% + å…³é”®è¯ 30%ï¼‰")
            else:
                final_scores = semantic_similarities
            
            # è·å– top-kï¼ˆæ‰©å¤§å€™é€‰é›†ï¼Œåç»­è¿‡æ»¤ï¼‰
            candidate_k = min(top_k * 2, len(final_scores))
            top_local_indices = np.argsort(final_scores)[-candidate_k:][::-1]
            top_indices = [protocol_indices[i] for i in top_local_indices]
            top_similarities = semantic_similarities[top_local_indices]
            top_scores = final_scores[top_local_indices]
        else:
            # æ£€ç´¢æ‰€æœ‰è§„åˆ™
            if self.use_faiss and self.faiss_index:
                # ä½¿ç”¨ FAISS åŠ é€Ÿæ£€ç´¢
                candidate_k = min(top_k * 2, len(self.rule_index))
                similarities, top_indices = self.faiss_index.search(
                    query_vector.reshape(1, -1).astype('float32'),
                    candidate_k
                )
                top_similarities = similarities[0]
                top_indices = top_indices[0]
                top_scores = top_similarities  # æš‚ä¸æ”¯æŒå…¨å±€æ··åˆæ£€ç´¢
            else:
                # ç›´æ¥è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                semantic_similarities = np.dot(self.rule_vectors, query_vector)
                
                if use_hybrid:
                    keyword_scores = np.array([
                        self._keyword_match_score(text, item["rule"])
                        for item in self.rule_index
                    ])
                    final_scores = 0.7 * semantic_similarities + 0.3 * keyword_scores
                else:
                    final_scores = semantic_similarities
                
                candidate_k = min(top_k * 2, len(final_scores))
                top_indices = np.argsort(final_scores)[-candidate_k:][::-1]
                top_similarities = semantic_similarities[top_indices]
                top_scores = final_scores[top_indices]
        
        # æ„é€ ç»“æœï¼ˆåº”ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼ï¼‰
        results = []
        for idx, similarity, score in zip(top_indices, top_similarities, top_scores):
            # åº”ç”¨åŠ¨æ€é˜ˆå€¼
            item = self.rule_index[idx]
            rule = item["rule"]
            
            # æ ¹æ®è§„åˆ™ç±»å‹è°ƒæ•´é˜ˆå€¼
            adjusted_threshold = self._get_adaptive_threshold(rule.check_type, min_similarity)
            
            if similarity >= adjusted_threshold:
                results.append({
                    "rule_id": rule.rule_id,
                    "category": item["category"],
                    "description": rule.description,
                    "check_type": rule.check_type,
                    "keywords": rule.keywords,
                    "positive_examples": rule.positive_examples,
                    "negative_examples": rule.negative_examples,
                    "severity": rule.severity,
                    "similarity": float(similarity),
                    "hybrid_score": float(score) if use_hybrid else float(similarity)
                })
                
                logger.debug(f"   âœ… è§„åˆ™ {rule.rule_id}: {rule.description[:30]}... (è¯­ä¹‰: {similarity:.3f}, ç»¼åˆ: {score:.3f})")
            else:
                logger.debug(f"   âš ï¸  è§„åˆ™ {rule.rule_id} ç›¸ä¼¼åº¦è¿‡ä½ ({similarity:.3f} < {adjusted_threshold:.3f})ï¼Œå·²è¿‡æ»¤")
            
            # åªè¿”å› top_k ä¸ª
            if len(results) >= top_k:
                break
        
        if not results:
            logger.warning(f"âŒ æ²¡æœ‰æ£€ç´¢åˆ°ä»»ä½•è§„åˆ™ï¼ˆé˜ˆå€¼: {min_similarity}ï¼‰")
        else:
            logger.info(f"âœ… æ£€ç´¢åˆ° {len(results)} æ¡ç›¸å…³è§„åˆ™")
        
        return results
    
    def _keyword_match_score(self, text: str, rule: Rule) -> float:
        """
        è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
        
        Args:
            text: å¾…æ£€ç´¢æ–‡æœ¬
            rule: è§„åˆ™å¯¹è±¡
        
        Returns:
            åŒ¹é…åˆ†æ•° [0, 1]
        """
        if not rule.keywords:
            return 0.0
        
        text_lower = text.lower()
        matched = sum(1 for keyword in rule.keywords if keyword.lower() in text_lower)
        
        return matched / len(rule.keywords)
    
    def _get_adaptive_threshold(self, check_type: str, base_threshold: float) -> float:
        """
        æ ¹æ®è§„åˆ™ç±»å‹åŠ¨æ€è°ƒæ•´é˜ˆå€¼
        
        Args:
            check_type: è§„åˆ™ç±»å‹ï¼ˆformat/semantic/structureï¼‰
            base_threshold: åŸºç¡€é˜ˆå€¼
        
        Returns:
            è°ƒæ•´åçš„é˜ˆå€¼
        """
        # format ç±»è§„åˆ™ï¼ˆæ ¼å¼æ£€æŸ¥ï¼‰è¦æ±‚æ›´ä¸¥æ ¼
        if check_type == "format":
            return base_threshold + 0.1
        # semantic ç±»è§„åˆ™ï¼ˆè¯­ä¹‰æ£€æŸ¥ï¼‰å¯ä»¥å®½æ¾ä¸€äº›
        elif check_type == "semantic":
            return base_threshold - 0.05
        # structure ç±»è§„åˆ™ï¼ˆç»“æ„æ£€æŸ¥ï¼‰æ ‡å‡†é˜ˆå€¼
        else:
            return base_threshold
    
    def get_all_rules_by_protocol(self, protocol_id: str) -> List[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šåè®®çš„æ‰€æœ‰è§„åˆ™
        
        Args:
            protocol_id: åè®®ID
        
        Returns:
            è§„åˆ™åˆ—è¡¨
        """
        if protocol_id not in self.standards:
            logger.warning(f"åè®®ä¸å­˜åœ¨: {protocol_id}")
            return []
        
        standard = self.standards[protocol_id]
        rules = []
        
        for category in standard.categories:
            for rule in category.rules:
                rules.append({
                    "rule_id": rule.rule_id,
                    "category": category.category,
                    "description": rule.description,
                    "check_type": rule.check_type,
                    "keywords": rule.keywords,
                    "positive_examples": rule.positive_examples,
                    "negative_examples": rule.negative_examples,
                    "severity": rule.severity
                })
        
        return rules
    
    def list_available_protocols(self) -> List[Dict[str, str]]:
        """
        åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„åè®®
        
        Returns:
            åè®®åˆ—è¡¨
        """
        return [
            {
                "protocol_id": std.protocol_id,
                "name": std.name,
                "version": std.version,
                "description": std.description or ""
            }
            for std in self.standards.values()
        ]
    
    def save_index(self, file_path: str = "standards/embeddings/index_v2.pkl"):
        """ä¿å­˜å‘é‡ç´¢å¼•ï¼ˆåŠ é€Ÿå¯åŠ¨ï¼‰"""
        try:
            save_dir = Path(file_path).parent
            save_dir.mkdir(parents=True, exist_ok=True)
            
            with open(file_path, 'wb') as f:
                pickle.dump({
                    "rule_vectors": self.rule_vectors,
                    "rule_index": self.rule_index,
                    "model_name": self.model_name
                }, f)
            
            # ä¿å­˜ FAISS ç´¢å¼•
            if self.use_faiss and self.faiss_index:
                faiss_path = str(file_path).replace('.pkl', '.faiss')
                faiss.write_index(self.faiss_index, faiss_path)
                logger.info(f"FAISS ç´¢å¼•å·²ä¿å­˜: {faiss_path}")
            
            logger.info(f"å‘é‡ç´¢å¼•å·²ä¿å­˜: {file_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜å‘é‡ç´¢å¼•å¤±è´¥: {e}")
    
    def load_index(self, file_path: str = "standards/embeddings/index_v2.pkl"):
        """åŠ è½½å‘é‡ç´¢å¼•ï¼ˆè·³è¿‡æ¨¡å‹åŠ è½½å’Œå‘é‡åŒ–ï¼‰"""
        try:
            with open(file_path, 'rb') as f:
                data = pickle.load(f)
                self.rule_vectors = data["rule_vectors"]
                self.rule_index = data["rule_index"]
                saved_model_name = data.get("model_name")
                
                if saved_model_name != self.model_name:
                    logger.warning(f"ç´¢å¼•ä½¿ç”¨çš„æ¨¡å‹ ({saved_model_name}) ä¸å½“å‰æ¨¡å‹ ({self.model_name}) ä¸åŒ")
            
            # åŠ è½½ FAISS ç´¢å¼•
            if self.use_faiss:
                faiss_path = str(file_path).replace('.pkl', '.faiss')
                if Path(faiss_path).exists():
                    self.faiss_index = faiss.read_index(faiss_path)
                    logger.info(f"FAISS ç´¢å¼•å·²åŠ è½½: {faiss_path}")
            
            logger.info(f"å‘é‡ç´¢å¼•å·²åŠ è½½: {file_path}")
        except Exception as e:
            logger.error(f"åŠ è½½å‘é‡ç´¢å¼•å¤±è´¥: {e}")
            logger.info("å°†é‡æ–°æ„å»ºç´¢å¼•...")
            self._build_vector_index()

